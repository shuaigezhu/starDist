{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarDistDeepmind.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuaigezhu/starDist/blob/master/StarDistDeepmind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4aG1djjj7rT",
        "colab_type": "text"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTjtw-Zsez8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97ae76a5-f071-4853-d411-72e466752759"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from math import pi\n",
        "from random import randint\n",
        "from torch import nn\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from sklearn import preprocessing\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "import glob"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdqYHyYEpTwM",
        "colab_type": "text"
      },
      "source": [
        "### **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH9QoZDGpVsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalization(data):\n",
        "  data_f = data.astype(float)\n",
        "  data_mean = np.mean(data_f, axis=0, keepdims=True)\n",
        "  data_n = data_f - data_mean\n",
        "  data_range = np.max(np.abs(data_n), axis=0, keepdims=True)\n",
        "  data_n = data_n / data_range\n",
        "  \n",
        "  return data_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-cCsbQcpxt2",
        "colab_type": "text"
      },
      "source": [
        "### **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btKhm_jCp0DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_mlp(input, output_sizes, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "  batch_size, _, filter_size = input.shape.as_list()\n",
        "  output = tf.reshape(input, (-1, filter_size))\n",
        "  output.set_shape((None, filter_size))\n",
        "\n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(output_sizes[:-1]):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(\n",
        "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  # Bring back into original shape\n",
        "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TfJH0Ixp2JZ",
        "colab_type": "text"
      },
      "source": [
        "### **Deterministic Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5JQ9GFtp45m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicEncoder(object):\n",
        "  \"\"\"The Deterministic Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, attention):\n",
        "    \"\"\"(A)NP deterministic encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      attention: The attention module.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._attention = attention\n",
        "\n",
        "  def __call__(self, context_x, context_y, target_x):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "      target_x: Tensor of shape [B,target_observations,d_x]. \n",
        "          For this 1D regression task this corresponds to the x-values.\n",
        "\n",
        "    Returns:\n",
        "      The encoded representation. Tensor of shape [B,target_observations,d]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \n",
        "                       \"deterministic_encoder\")\n",
        "\n",
        "    # Apply attention\n",
        "    with tf.variable_scope(\"deterministic_encoder\", reuse=tf.AUTO_REUSE):\n",
        "        hidden = self._attention(context_x, target_x, hidden)\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPY2dwOJp60G",
        "colab_type": "text"
      },
      "source": [
        "### **Latent Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jP_BGccp902",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentEncoder(object):\n",
        "  \"\"\"The Latent Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, num_latents):\n",
        "    \"\"\"(A)NP latent encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      num_latents: The latent dimensionality.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._num_latents = num_latents\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "\n",
        "    Returns:\n",
        "      A normal distribution over tensors of shape [B, num_latents]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([x, y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \"latent_encoder\")\n",
        "      \n",
        "    # Aggregator: take the mean over all points\n",
        "    hidden = tf.reduce_mean(hidden, axis=1)\n",
        "    \n",
        "    # Have further MLP layers that map to the parameters of the Gaussian latent\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self._output_sizes[-1] + self._num_latents)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self._num_latents, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self._num_latents, name=\"std_layer\")\n",
        "      \n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTaVxH-qAu7",
        "colab_type": "text"
      },
      "source": [
        "### **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt59dKLOqDrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes):\n",
        "    \"\"\"(A)NP decoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
        "          as defined in `basic.Linear`.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "\n",
        "  def __call__(self, representation, target_x):\n",
        "    \"\"\"Decodes the individual targets.\n",
        "\n",
        "    Args:\n",
        "      representation: The representation of the context for target predictions. \n",
        "          Tensor of shape [B,target_observations,?].\n",
        "      target_x: The x locations for the target query.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "\n",
        "    Returns:\n",
        "      dist: A multivariate Gaussian over the target points. A distribution over\n",
        "          tensors of shape [B,target_observations,d_y].\n",
        "      mu: The mean of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "      sigma: The standard deviation of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "    \"\"\"\n",
        "    # concatenate target_x and representation\n",
        "    hidden = tf.concat([representation, target_x], axis=-1)\n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(hidden, self._output_sizes, \"decoder\")\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "\n",
        "    # Get the distribution\n",
        "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
        "        loc=mu, scale_diag=sigma)\n",
        "\n",
        "    return dist, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HflFnTYaqJSw",
        "colab_type": "text"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwL3d6r8qMI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentModel(object):\n",
        "  \"\"\"The (A)NP model.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_encoder_output_sizes, num_latents,\n",
        "               decoder_output_sizes, use_deterministic_path=True, \n",
        "               deterministic_encoder_output_sizes=None, attention=None):\n",
        "    \"\"\"Initialises the model.\n",
        "\n",
        "    Args:\n",
        "      latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
        "          layers of the latent encoder.\n",
        "      num_latents: The latent dimensionality.\n",
        "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the decoder. The last element should correspond to d_y * 2\n",
        "          (it encodes both mean and variance concatenated)\n",
        "      use_deterministic_path: a boolean that indicates whether the deterministic\n",
        "          encoder is used or not.\n",
        "      deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
        "          hidden layers of the deterministic encoder. The last one is the size \n",
        "          of the deterministic representation r.\n",
        "      attention: The attention module used in the deterministic encoder.\n",
        "          Only relevant when use_deterministic_path=True.\n",
        "    \"\"\"\n",
        "    self._latent_encoder = LatentEncoder(latent_encoder_output_sizes, \n",
        "                                         num_latents)\n",
        "    self._decoder = Decoder(decoder_output_sizes)\n",
        "    self._use_deterministic_path = use_deterministic_path\n",
        "    if use_deterministic_path:\n",
        "      self._deterministic_encoder = DeterministicEncoder(\n",
        "          deterministic_encoder_output_sizes, attention)\n",
        "    \n",
        "\n",
        "  def __call__(self, query, num_targets, target_y=None):\n",
        "    \"\"\"Returns the predicted mean and variance at the target points.\n",
        "\n",
        "    Args:\n",
        "      query: Array containing ((context_x, context_y), target_x) where:\n",
        "          context_x: Tensor of shape [B,num_contexts,d_x]. \n",
        "              Contains the x values of the context points.\n",
        "          context_y: Tensor of shape [B,num_contexts,d_y]. \n",
        "              Contains the y values of the context points.\n",
        "          target_x: Tensor of shape [B,num_targets,d_x]. \n",
        "              Contains the x values of the target points.\n",
        "      num_targets: Number of target points.\n",
        "      target_y: The ground truth y values of the target y. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "\n",
        "    Returns:\n",
        "      log_p: The log_probability of the target_y given the predicted\n",
        "          distribution. Tensor of shape [B,num_targets].\n",
        "      mu: The mean of the predicted distribution. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "      sigma: The variance of the predicted distribution.\n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "    \"\"\"\n",
        "\n",
        "    (context_x, context_y), target_x = query\n",
        "\n",
        "    # Pass query through the encoder and the decoder\n",
        "    prior = self._latent_encoder(context_x, context_y)\n",
        "    \n",
        "    # For training, when target_y is available, use targets for latent encoder.\n",
        "    # Note that targets contain contexts by design.\n",
        "    if target_y is None:\n",
        "      latent_rep = prior.sample()\n",
        "    # For testing, when target_y unavailable, use contexts for latent encoder.\n",
        "    else:\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      latent_rep = posterior.sample()\n",
        "    latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n",
        "                         [1, num_targets, 1])\n",
        "    if self._use_deterministic_path:\n",
        "      deterministic_rep = self._deterministic_encoder(context_x, context_y,\n",
        "                                                      target_x)\n",
        "      representation = tf.concat([deterministic_rep, latent_rep], axis=-1)\n",
        "    else:\n",
        "      representation = latent_rep\n",
        "      \n",
        "    dist, mu, sigma = self._decoder(representation, target_x)\n",
        "    \n",
        "    # If we want to calculate the log_prob for training we will make use of the\n",
        "    # target_y. At test time the target_y is not available so we return None.\n",
        "    if target_y is not None:\n",
        "      log_p = dist.log_prob(target_y)\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      kl = tf.reduce_sum(\n",
        "          tf.contrib.distributions.kl_divergence(posterior, prior), \n",
        "          axis=-1, keepdims=True)\n",
        "      kl = tf.tile(kl, [1, num_targets])\n",
        "      loss = - tf.reduce_mean(log_p - kl / tf.cast(num_targets, tf.float32))\n",
        "    else:\n",
        "      log_p = None\n",
        "      kl = None\n",
        "      loss = None\n",
        "\n",
        "    return mu, sigma, log_p, kl, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weoCLguUqP1M",
        "colab_type": "text"
      },
      "source": [
        "### **Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzHgDZ3pqSBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uniform_attention(q, v):\n",
        "  \"\"\"Uniform attention. Equivalent to np.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  total_points = tf.shape(q)[1]\n",
        "  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
        "  rep = tf.tile(rep, [1, total_points, 1])\n",
        "  return rep\n",
        "\n",
        "def laplace_attention(q, k, v, scale, normalise):\n",
        "  \"\"\"Computes laplace exponential attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    scale: float that scales the L1 distance.\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  k = tf.expand_dims(k, axis=1)  # [B,1,n,d_k]\n",
        "  q = tf.expand_dims(q, axis=2)  # [B,m,1,d_k]\n",
        "  unnorm_weights = - tf.abs((k - q) / scale)  # [B,m,n,d_k]\n",
        "  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = lambda x: 1 + tf.tanh(x)\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def dot_product_attention(q, k, v, normalise):\n",
        "  \"\"\"Computes dot product attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = tf.shape(q)[-1]\n",
        "  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "  unnorm_weights = tf.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = tf.sigmoid\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def multihead_attention(q, k, v, num_heads=8):\n",
        "  \"\"\"Computes multi-head attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    num_heads: number of heads. Should divide d_v.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = q.get_shape().as_list()[-1]\n",
        "  d_v = v.get_shape().as_list()[-1]\n",
        "  head_size = d_v / num_heads\n",
        "  key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n",
        "  value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n",
        "  rep = tf.constant(0.0)\n",
        "  for h in range(num_heads):\n",
        "    o = dot_product_attention(\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wq%d' % h, use_bias=False, padding='VALID')(q),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wk%d' % h, use_bias=False, padding='VALID')(k),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wv%d' % h, use_bias=False, padding='VALID')(v),\n",
        "        normalise=True)\n",
        "    rep += tf.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n",
        "                      name='wo%d' % h, use_bias=False, padding='VALID')(o)\n",
        "  return rep\n",
        "\n",
        "class Attention(object):\n",
        "  \"\"\"The Attention module.\"\"\"\n",
        "\n",
        "  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n",
        "               num_heads=8):\n",
        "    \"\"\"Create attention module.\n",
        "\n",
        "    Takes in context inputs, target inputs and\n",
        "    representations of each context input/output pair\n",
        "    to output an aggregated representation of the context data.\n",
        "    Args:\n",
        "      rep: transformation to apply to contexts before computing attention. \n",
        "          One of: ['identity','mlp'].\n",
        "      output_sizes: list of number of hidden units per layer of mlp.\n",
        "          Used only if rep == 'mlp'.\n",
        "      att_type: type of attention. One of the following:\n",
        "          ['uniform','laplace','dot_product','multihead']\n",
        "      scale: scale of attention.\n",
        "      normalise: Boolean determining whether to:\n",
        "          1. apply softmax to weights so that they sum to 1 across context pts or\n",
        "          2. apply custom transformation to have weights in [0,1].\n",
        "      num_heads: number of heads for multihead.\n",
        "    \"\"\"\n",
        "    self._rep = rep\n",
        "    self._output_sizes = output_sizes\n",
        "    self._type = att_type\n",
        "    self._scale = scale\n",
        "    self._normalise = normalise\n",
        "    if self._type == 'multihead':\n",
        "      self._num_heads = num_heads\n",
        "\n",
        "  def __call__(self, x1, x2, r):\n",
        "    \"\"\"Apply attention to create aggregated representation of r.\n",
        "\n",
        "    Args:\n",
        "      x1: tensor of shape [B,n1,d_x].\n",
        "      x2: tensor of shape [B,n2,d_x].\n",
        "      r: tensor of shape [B,n1,d].\n",
        "      \n",
        "    Returns:\n",
        "      tensor of shape [B,n2,d]\n",
        "\n",
        "    Raises:\n",
        "      NameError: The argument for rep/type was invalid.\n",
        "    \"\"\"\n",
        "    if self._rep == 'identity':\n",
        "      k, q = (x1, x2)\n",
        "    elif self._rep == 'mlp':\n",
        "      # Pass through MLP\n",
        "      k = batch_mlp(x1, self._output_sizes, \"attention\")\n",
        "      q = batch_mlp(x2, self._output_sizes, \"attention\")\n",
        "    else:\n",
        "      raise NameError(\"'rep' not among ['identity','mlp']\")\n",
        "\n",
        "    if self._type == 'uniform':\n",
        "      rep = uniform_attention(q, r)\n",
        "    elif self._type == 'laplace':\n",
        "      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
        "    elif self._type == 'dot_product':\n",
        "      rep = dot_product_attention(q, k, r, self._normalise)\n",
        "    elif self._type == 'multihead':\n",
        "      rep = multihead_attention(q, k, r, self._num_heads)\n",
        "    else:\n",
        "      raise NameError((\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
        "                       \",'multihead']\"))\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dandQrbZkFxa",
        "colab_type": "text"
      },
      "source": [
        "### **Data wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12lHyMJHkILa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)[:,:num_context+num_target,:]\n",
        "  target_y = np.array(batch_datay)[:,:num_context+num_target,:]\n",
        "  \n",
        "    #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = num_context+num_target\n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points\n",
        "\n",
        "def testdatawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]#context is belong to target\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)#x values\n",
        "  target_y = np.array(batch_datay)#y values\n",
        "  \n",
        "  #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = batch_size \n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0-dC0pao2g-",
        "colab_type": "text"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JihmwfP7o_jx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7f94888d-1b22-47e8-f5a3-7846601ea3aa"
      },
      "source": [
        "\n",
        "\n",
        "#!unzip -q \"/content/gaia_pos.zip\"\n",
        "# Get data file names\n",
        "path =r'/content/gaia_pos'\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "error_dfs = []\n",
        "\n",
        "# Select specific colomns which are \"ra\", \"dec\", \"magnitude\", \"wavelength\" and \"parallax\" and \"parallax error\"\n",
        "\n",
        "for filename in filenames:\n",
        "  data = pd.read_csv(filename, header=None)\n",
        "  select_data = data.iloc[1:,[2, 4, 10, 11, 6]]\n",
        "  parallax_error = data.iloc[1:,[7]]\n",
        "  \n",
        "  dfs.append(select_data)\n",
        "  error_dfs.append(parallax_error)\n",
        "\n",
        "# Combine the three files of the different type of filter into one pandas framework\n",
        "\n",
        "frame = pd.concat(dfs, axis=0, ignore_index=True, sort = False)\n",
        "parallax_error_frame = pd.concat(error_dfs, axis=0, ignore_index=True, sort = False)\n",
        "\n",
        "frame = frame.rename(columns={2: \"ra\", 4: \"dec\", 10: \"magnitude\", 11: \"wavelength\", 6: \"parallax\"})\n",
        "parallax_error_frame = parallax_error_frame.rename(columns={7:\"parallax error\"})\n",
        "#print(parallax_error_frame.head())\n",
        "\n",
        "# Split the dataset into training and testing part.\n",
        "ratio = 0.9 #the ratio is train/all \n",
        "msk = np.random.rand(len(frame)) < ratio\n",
        "train_frame = frame[msk]\n",
        "test_frame = frame[~msk]\n",
        "parallax_error_test_frame = parallax_error_frame[~msk]\n",
        "\n",
        "def split_into_xy(dataframe):  \n",
        "  x = dataframe.iloc[:, [0,1,2,3]]\n",
        "  y = dataframe.iloc[:, [4]]\n",
        "  \n",
        "  return x , y\n",
        "\n",
        "train_x, train_y = split_into_xy(train_frame)\n",
        "test_x, test_y = split_into_xy(test_frame)\n",
        "\n",
        "#convert into numpy array and type of float\n",
        "train_x = np.array(train_x)\n",
        "train_x = train_x.astype(float)\n",
        "train_y = np.array(train_y)\n",
        "train_y = train_y.astype(float)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_x = test_x.astype(float)\n",
        "test_y = np.array(test_y)\n",
        "test_y = test_y.astype(float)\n",
        "\n",
        "test_parallax_error = np.array(parallax_error_test_frame).astype(float)\n",
        "\n",
        "locations = np.random.choice(train_x.shape[0],\n",
        "                                 size=700000,\n",
        "                                 replace=False)\n",
        "\n",
        "r_scaler = preprocessing.RobustScaler()\n",
        "train_norm_x = Normalization(train_x)[locations,:]\n",
        "train_norm_y = Normalization(train_y)[locations,:]\n",
        "#train_norm_y = r_scaler.fit_transform(train_y)\n",
        "test_norm_x = Normalization(test_x)\n",
        "test_norm_y = Normalization(test_y)\n",
        "#test_norm_y = r_scaler.fit_transform(test_y)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Db63a7rpHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d76e1d3e-bb33-4fb4-d91e-cf7761791eac"
      },
      "source": [
        "print(train_norm_x.shape)\n",
        "print(test_norm_x.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(700000, 4)\n",
            "(90423, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtoiMoH61avg",
        "colab_type": "text"
      },
      "source": [
        "### **Data Preprocessing 2 (only mag and lamda)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9PVKTZr1hxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cda40dab-6037-4816-ea8d-dcdf16e904e2"
      },
      "source": [
        "#!unzip -q \"/content/gaia_pos.zip\"\n",
        "# Get data file names\n",
        "path =r'/content/gaia_pos'\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "error_dfs = []\n",
        "\n",
        "# Select specific colomns which are \"ra\", \"dec\", \"magnitude\", \"wavelength\" and \"parallax\" and \"parallax error\"\n",
        "\n",
        "for filename in filenames:\n",
        "  data = pd.read_csv(filename, header=None)\n",
        "  select_data = data.iloc[1:,[10, 11, 6]]\n",
        "  parallax_error = data.iloc[1:,[7]]\n",
        "  \n",
        "  dfs.append(select_data)\n",
        "  error_dfs.append(parallax_error)\n",
        "\n",
        "# Combine the three files of the different type of filter into one pandas framework\n",
        "\n",
        "frame = pd.concat(dfs, axis=0, ignore_index=True, sort = False)\n",
        "parallax_error_frame = pd.concat(error_dfs, axis=0, ignore_index=True, sort = False)\n",
        "\n",
        "frame = frame.rename(columns={10: \"magnitude\", 11: \"wavelength\", 6: \"parallax\"})\n",
        "parallax_error_frame = parallax_error_frame.rename(columns={7:\"parallax error\"})\n",
        "#print(parallax_error_frame.head())\n",
        "\n",
        "# Split the dataset into training and testing part.\n",
        "ratio = 0.99 #the ratio is train/all \n",
        "msk = np.random.rand(len(frame)) < ratio\n",
        "train_frame = frame[msk]\n",
        "test_frame = frame[~msk]\n",
        "parallax_error_test_frame = parallax_error_frame[~msk]\n",
        "\n",
        "def split_into_xy(dataframe):  \n",
        "  x = dataframe.iloc[:, [0,1]]\n",
        "  y = dataframe.iloc[:, [2]]\n",
        "  \n",
        "  return x , y\n",
        "\n",
        "train_x, train_y = split_into_xy(train_frame)\n",
        "test_x, test_y = split_into_xy(test_frame)\n",
        "\n",
        "#convert into numpy array and type of float\n",
        "train_x = np.array(train_x)\n",
        "train_x = train_x.astype(float)\n",
        "train_y = np.array(train_y)\n",
        "train_y = train_y.astype(float)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_x = test_x.astype(float)\n",
        "test_y = np.array(test_y)\n",
        "test_y = test_y.astype(float)\n",
        "\n",
        "test_parallax_error = np.array(parallax_error_test_frame).astype(float)\n",
        "\n",
        "locations = np.random.choice(train_x.shape[0],\n",
        "                                 size=25000,\n",
        "                                 replace=False)\n",
        "\n",
        "r_scaler = preprocessing.RobustScaler()\n",
        "train_norm_x = Normalization(train_x)[locations,:]\n",
        "train_norm_y = Normalization(train_y)[locations,:]\n",
        "#train_norm_y = r_scaler.fit_transform(train_y)\n",
        "test_norm_x = Normalization(test_x)\n",
        "test_norm_y = Normalization(test_y)\n",
        "#test_norm_y = r_scaler.fit_transform(test_y)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJYH1Le32Kdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "367f28a5-2c32-491e-d93f-0dcda947d119"
      },
      "source": [
        "print(train_norm_x.shape)\n",
        "print(test_norm_x.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(700000, 4)\n",
            "(90423, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FkTpJs2ngvA",
        "colab_type": "text"
      },
      "source": [
        "### **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oBhSjhpnjTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c2c1f2c0-81b9-4f4c-e08f-4129a59ebfb2"
      },
      "source": [
        "TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
        "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
        "PLOT_AFTER = 500 #@param {type:\"number\"}\n",
        "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
        "MODEL_TYPE = 'ANP' #@param ['NP','ANP']\n",
        "ATTENTION_TYPE = 'multihead' #@param ['uniform','laplace','dot_product','multihead']\n",
        "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Sizes of the layers of the MLPs for the encoders and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
        "num_latents = HIDDEN_SIZE\n",
        "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
        "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
        "use_deterministic_path = True\n",
        "\n",
        "# ANP with multihead attention\n",
        "if MODEL_TYPE == 'ANP':\n",
        "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
        "                        att_type='multihead')\n",
        "# NP - equivalent to uniform attention\n",
        "elif MODEL_TYPE == 'NP':\n",
        "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
        "else:\n",
        "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
        "\n",
        "# Define the model\n",
        "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
        "                    decoder_output_sizes, use_deterministic_path, \n",
        "                    deterministic_encoder_output_sizes, attention)\n",
        "\n",
        "# Define data\n",
        "batch_size = 100\n",
        "train_query, train_target_y, train_num_total_points, train_num_context_points = datawrap(train_norm_x, train_norm_y, batch_size)\n",
        "\n",
        "test_query, test_target_y, test_num_total_points, test_num_context_points = testdatawrap(test_norm_x, test_norm_y, test_norm_x.shape[0])\n",
        "\n",
        "# Define the loss\n",
        "_, _, log_prob, _, loss = model(train_query, train_num_total_points,\n",
        "                                 train_target_y)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "mu, sigma, _, _, _ = model(test_query, test_num_total_points)\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-5)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Train and plot\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PLOT_AFTER == 0:\n",
        "      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, test_target_y, \n",
        "           test_query])\n",
        "      print(\"The accuracy is .....\")\n",
        "      accuracy = abs(pred_y-target_y)/target_y\n",
        "      accuracy = np.reshape(accuracy, accuracy.shape[1])\n",
        "      print(np.median(accuracy))\n",
        "      \n",
        "      print(\"The prediction is ....\")\n",
        "      print(pred_y[:,300:305,:])\n",
        "      print(\"The label is ....\")\n",
        "      print(target_y[:,300:305,:])\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
        "\n",
        "      # Plot the prediction and the context\n",
        "      #plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d1e297859b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrandom_kernel_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;31m#@param {type:\"boolean\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Sizes of the layers of the MLPs for the encoders and decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q_lFVtRweZX",
        "colab_type": "text"
      },
      "source": [
        "### **visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjywlHu8tCHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}