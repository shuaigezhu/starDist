{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GaiaData.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuaigezhu/starDist/blob/master/GaiaData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE9n8cHU42te",
        "colab_type": "text"
      },
      "source": [
        "### **Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NxKr2tG2Xxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29753d58-d148-4a36-9a6d-8d8f6aa496b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from math import pi\n",
        "from random import randint\n",
        "from torch import nn\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from sklearn import preprocessing\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "import glob"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDVseLIcQiec",
        "colab_type": "text"
      },
      "source": [
        "### **Normalizaton**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrdLBESHQlBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalization(data):\n",
        "  data_f = data.astype(float)\n",
        "  data_mean = np.mean(data_f, axis=0, keepdims=True)\n",
        "  data_n = data_f - data_mean\n",
        "  data_range = np.max(np.abs(data_n), axis=0, keepdims=True)\n",
        "  data_n = data_n / data_range\n",
        "  \n",
        "  return data_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqLtB5FpQp5P",
        "colab_type": "text"
      },
      "source": [
        "### **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad-9qxPQrfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_mlp(input, output_sizes, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "  batch_size, _, filter_size = input.shape.as_list()\n",
        "  output = tf.reshape(input, (-1, filter_size))\n",
        "  output.set_shape((None, filter_size))\n",
        "\n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(output_sizes[:-1]):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(\n",
        "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  # Bring back into original shape\n",
        "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4gbrt8GQvop",
        "colab_type": "text"
      },
      "source": [
        "### **Deterministic Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33yM5QspQw1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicEncoder(object):\n",
        "  \"\"\"The Deterministic Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, attention):\n",
        "    \"\"\"(A)NP deterministic encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      attention: The attention module.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._attention = attention\n",
        "\n",
        "  def __call__(self, context_x, context_y, target_x):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "      target_x: Tensor of shape [B,target_observations,d_x]. \n",
        "          For this 1D regression task this corresponds to the x-values.\n",
        "\n",
        "    Returns:\n",
        "      The encoded representation. Tensor of shape [B,target_observations,d]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \n",
        "                       \"deterministic_encoder\")\n",
        "\n",
        "    # Apply attention\n",
        "    with tf.variable_scope(\"deterministic_encoder\", reuse=tf.AUTO_REUSE):\n",
        "        hidden = self._attention(context_x, target_x, hidden)\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCvh1T2fQzvL",
        "colab_type": "text"
      },
      "source": [
        "### **Latent Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkzjqIZ-Q3yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentEncoder(object):\n",
        "  \"\"\"The Latent Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, num_latents):\n",
        "    \"\"\"(A)NP latent encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      num_latents: The latent dimensionality.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._num_latents = num_latents\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "\n",
        "    Returns:\n",
        "      A normal distribution over tensors of shape [B, num_latents]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([x, y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \"latent_encoder\")\n",
        "      \n",
        "    # Aggregator: take the mean over all points\n",
        "    hidden = tf.reduce_mean(hidden, axis=1)\n",
        "    \n",
        "    # Have further MLP layers that map to the parameters of the Gaussian latent\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self._output_sizes[-1] + self._num_latents)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self._num_latents, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self._num_latents, name=\"std_layer\")\n",
        "      \n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch--M8kJQ7pU",
        "colab_type": "text"
      },
      "source": [
        "### **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23JwLJ7VQ_pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes):\n",
        "    \"\"\"(A)NP decoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
        "          as defined in `basic.Linear`.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "\n",
        "  def __call__(self, representation, target_x):\n",
        "    \"\"\"Decodes the individual targets.\n",
        "\n",
        "    Args:\n",
        "      representation: The representation of the context for target predictions. \n",
        "          Tensor of shape [B,target_observations,?].\n",
        "      target_x: The x locations for the target query.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "\n",
        "    Returns:\n",
        "      dist: A multivariate Gaussian over the target points. A distribution over\n",
        "          tensors of shape [B,target_observations,d_y].\n",
        "      mu: The mean of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "      sigma: The standard deviation of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "    \"\"\"\n",
        "    # concatenate target_x and representation\n",
        "    hidden = tf.concat([representation, target_x], axis=-1)\n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(hidden, self._output_sizes, \"decoder\")\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "\n",
        "    # Get the distribution\n",
        "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
        "        loc=mu, scale_diag=sigma)\n",
        "\n",
        "    return dist, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrkUSDPbRBz4",
        "colab_type": "text"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maUo6irYRE0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentModel(object):\n",
        "  \"\"\"The (A)NP model.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_encoder_output_sizes, num_latents,\n",
        "               decoder_output_sizes, use_deterministic_path=True, \n",
        "               deterministic_encoder_output_sizes=None, attention=None):\n",
        "    \"\"\"Initialises the model.\n",
        "\n",
        "    Args:\n",
        "      latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
        "          layers of the latent encoder.\n",
        "      num_latents: The latent dimensionality.\n",
        "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the decoder. The last element should correspond to d_y * 2\n",
        "          (it encodes both mean and variance concatenated)\n",
        "      use_deterministic_path: a boolean that indicates whether the deterministic\n",
        "          encoder is used or not.\n",
        "      deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
        "          hidden layers of the deterministic encoder. The last one is the size \n",
        "          of the deterministic representation r.\n",
        "      attention: The attention module used in the deterministic encoder.\n",
        "          Only relevant when use_deterministic_path=True.\n",
        "    \"\"\"\n",
        "    self._latent_encoder = LatentEncoder(latent_encoder_output_sizes, \n",
        "                                         num_latents)\n",
        "    self._decoder = Decoder(decoder_output_sizes)\n",
        "    self._use_deterministic_path = use_deterministic_path\n",
        "    if use_deterministic_path:\n",
        "      self._deterministic_encoder = DeterministicEncoder(\n",
        "          deterministic_encoder_output_sizes, attention)\n",
        "    \n",
        "\n",
        "  def __call__(self, query, num_targets, target_y=None):\n",
        "    \"\"\"Returns the predicted mean and variance at the target points.\n",
        "\n",
        "    Args:\n",
        "      query: Array containing ((context_x, context_y), target_x) where:\n",
        "          context_x: Tensor of shape [B,num_contexts,d_x]. \n",
        "              Contains the x values of the context points.\n",
        "          context_y: Tensor of shape [B,num_contexts,d_y]. \n",
        "              Contains the y values of the context points.\n",
        "          target_x: Tensor of shape [B,num_targets,d_x]. \n",
        "              Contains the x values of the target points.\n",
        "      num_targets: Number of target points.\n",
        "      target_y: The ground truth y values of the target y. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "\n",
        "    Returns:\n",
        "      log_p: The log_probability of the target_y given the predicted\n",
        "          distribution. Tensor of shape [B,num_targets].\n",
        "      mu: The mean of the predicted distribution. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "      sigma: The variance of the predicted distribution.\n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "    \"\"\"\n",
        "\n",
        "    (context_x, context_y), target_x = query\n",
        "\n",
        "    # Pass query through the encoder and the decoder\n",
        "    prior = self._latent_encoder(context_x, context_y)\n",
        "    \n",
        "    # For training, when target_y is available, use targets for latent encoder.\n",
        "    # Note that targets contain contexts by design.\n",
        "    if target_y is None:\n",
        "      latent_rep = prior.sample()\n",
        "    # For testing, when target_y unavailable, use contexts for latent encoder.\n",
        "    else:\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      latent_rep = posterior.sample()\n",
        "    latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n",
        "                         [1, num_targets, 1])\n",
        "    if self._use_deterministic_path:\n",
        "      deterministic_rep = self._deterministic_encoder(context_x, context_y,\n",
        "                                                      target_x)\n",
        "      representation = tf.concat([deterministic_rep, latent_rep], axis=-1)\n",
        "    else:\n",
        "      representation = latent_rep\n",
        "      \n",
        "    dist, mu, sigma = self._decoder(representation, target_x)\n",
        "    \n",
        "    # If we want to calculate the log_prob for training we will make use of the\n",
        "    # target_y. At test time the target_y is not available so we return None.\n",
        "    if target_y is not None:\n",
        "      log_p = dist.log_prob(target_y)\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      kl = tf.reduce_sum(\n",
        "          tf.contrib.distributions.kl_divergence(posterior, prior), \n",
        "          axis=-1, keepdims=True)\n",
        "      kl = tf.tile(kl, [1, num_targets])\n",
        "      loss = - tf.reduce_mean(log_p - kl / tf.cast(num_targets, tf.float32))\n",
        "    else:\n",
        "      log_p = None\n",
        "      kl = None\n",
        "      loss = None\n",
        "\n",
        "    return mu, sigma, log_p, kl, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tuBBGcRIEj",
        "colab_type": "text"
      },
      "source": [
        "### **Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E4iitknRLR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uniform_attention(q, v):\n",
        "  \"\"\"Uniform attention. Equivalent to np.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  total_points = tf.shape(q)[1]\n",
        "  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
        "  rep = tf.tile(rep, [1, total_points, 1])\n",
        "  return rep\n",
        "\n",
        "def laplace_attention(q, k, v, scale, normalise):\n",
        "  \"\"\"Computes laplace exponential attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    scale: float that scales the L1 distance.\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  k = tf.expand_dims(k, axis=1)  # [B,1,n,d_k]\n",
        "  q = tf.expand_dims(q, axis=2)  # [B,m,1,d_k]\n",
        "  unnorm_weights = - tf.abs((k - q) / scale)  # [B,m,n,d_k]\n",
        "  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = lambda x: 1 + tf.tanh(x)\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def dot_product_attention(q, k, v, normalise):\n",
        "  \"\"\"Computes dot product attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = tf.shape(q)[-1]\n",
        "  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "  unnorm_weights = tf.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = tf.sigmoid\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def multihead_attention(q, k, v, num_heads=8):\n",
        "  \"\"\"Computes multi-head attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    num_heads: number of heads. Should divide d_v.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = q.get_shape().as_list()[-1]\n",
        "  d_v = v.get_shape().as_list()[-1]\n",
        "  head_size = d_v / num_heads\n",
        "  key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n",
        "  value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n",
        "  rep = tf.constant(0.0)\n",
        "  for h in range(num_heads):\n",
        "    o = dot_product_attention(\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wq%d' % h, use_bias=False, padding='VALID')(q),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wk%d' % h, use_bias=False, padding='VALID')(k),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wv%d' % h, use_bias=False, padding='VALID')(v),\n",
        "        normalise=True)\n",
        "    rep += tf.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n",
        "                      name='wo%d' % h, use_bias=False, padding='VALID')(o)\n",
        "  return rep\n",
        "\n",
        "class Attention(object):\n",
        "  \"\"\"The Attention module.\"\"\"\n",
        "\n",
        "  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n",
        "               num_heads=8):\n",
        "    \"\"\"Create attention module.\n",
        "\n",
        "    Takes in context inputs, target inputs and\n",
        "    representations of each context input/output pair\n",
        "    to output an aggregated representation of the context data.\n",
        "    Args:\n",
        "      rep: transformation to apply to contexts before computing attention. \n",
        "          One of: ['identity','mlp'].\n",
        "      output_sizes: list of number of hidden units per layer of mlp.\n",
        "          Used only if rep == 'mlp'.\n",
        "      att_type: type of attention. One of the following:\n",
        "          ['uniform','laplace','dot_product','multihead']\n",
        "      scale: scale of attention.\n",
        "      normalise: Boolean determining whether to:\n",
        "          1. apply softmax to weights so that they sum to 1 across context pts or\n",
        "          2. apply custom transformation to have weights in [0,1].\n",
        "      num_heads: number of heads for multihead.\n",
        "    \"\"\"\n",
        "    self._rep = rep\n",
        "    self._output_sizes = output_sizes\n",
        "    self._type = att_type\n",
        "    self._scale = scale\n",
        "    self._normalise = normalise\n",
        "    if self._type == 'multihead':\n",
        "      self._num_heads = num_heads\n",
        "\n",
        "  def __call__(self, x1, x2, r):\n",
        "    \"\"\"Apply attention to create aggregated representation of r.\n",
        "\n",
        "    Args:\n",
        "      x1: tensor of shape [B,n1,d_x].\n",
        "      x2: tensor of shape [B,n2,d_x].\n",
        "      r: tensor of shape [B,n1,d].\n",
        "      \n",
        "    Returns:\n",
        "      tensor of shape [B,n2,d]\n",
        "\n",
        "    Raises:\n",
        "      NameError: The argument for rep/type was invalid.\n",
        "    \"\"\"\n",
        "    if self._rep == 'identity':\n",
        "      k, q = (x1, x2)\n",
        "    elif self._rep == 'mlp':\n",
        "      # Pass through MLP\n",
        "      k = batch_mlp(x1, self._output_sizes, \"attention\")\n",
        "      q = batch_mlp(x2, self._output_sizes, \"attention\")\n",
        "    else:\n",
        "      raise NameError(\"'rep' not among ['identity','mlp']\")\n",
        "\n",
        "    if self._type == 'uniform':\n",
        "      rep = uniform_attention(q, r)\n",
        "    elif self._type == 'laplace':\n",
        "      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
        "    elif self._type == 'dot_product':\n",
        "      rep = dot_product_attention(q, k, r, self._normalise)\n",
        "    elif self._type == 'multihead':\n",
        "      rep = multihead_attention(q, k, r, self._num_heads)\n",
        "    else:\n",
        "      raise NameError((\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
        "                       \",'multihead']\"))\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex4ioJm3RTkj",
        "colab_type": "text"
      },
      "source": [
        "### **Data wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S5ISWwHRVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)[:,:num_context+num_target,:]\n",
        "  target_y = np.array(batch_datay)[:,:num_context+num_target,:]\n",
        "  \n",
        "    #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = num_context+num_target\n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points\n",
        "\n",
        "def testdatawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]#context is belong to target\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)#x values\n",
        "  target_y = np.array(batch_datay)#y values\n",
        "  \n",
        "  #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = batch_size \n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN1RJgd_49bw",
        "colab_type": "text"
      },
      "source": [
        "### **Data selector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T41267Nh48Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input your filename (csv.)\n",
        "filename = \"/content/all_gaia_yufengzhu.csv\"\n",
        "\n",
        "# all the features\n",
        "# solution_id,designation,source_id,random_index,ref_epoch,ra,ra_error,dec,dec_error,\n",
        "# parallax,parallax_error,parallax_over_error,pmra,pmra_error,pmdec,pmdec_error,\n",
        "# ra_dec_corr,ra_parallax_corr,ra_pmra_corr,ra_pmdec_corr,dec_parallax_corr,dec_pmra_corr,dec_pmdec_corr,\n",
        "# parallax_pmra_corr,parallax_pmdec_corr,pmra_pmdec_corr,astrometric_n_obs_al,astrometric_n_obs_ac,\n",
        "# astrometric_n_good_obs_al,astrometric_n_bad_obs_al,astrometric_gof_al,astrometric_chi2_al,astrometric_excess_noise,\n",
        "# astrometric_excess_noise_sig,astrometric_params_solved,astrometric_primary_flag,astrometric_weight_al,\n",
        "# astrometric_pseudo_colour,astrometric_pseudo_colour_error,mean_varpi_factor_al,astrometric_matched_observations,\n",
        "# visibility_periods_used,astrometric_sigma5d_max,frame_rotator_object_type,matched_observations,duplicated_source,\n",
        "# phot_g_n_obs,phot_g_mean_flux,phot_g_mean_flux_error,phot_g_mean_flux_over_error,phot_g_mean_mag,\n",
        "# phot_bp_n_obs,phot_bp_mean_flux,phot_bp_mean_flux_error,phot_bp_mean_flux_over_error,phot_bp_mean_mag,\n",
        "# phot_rp_n_obs,phot_rp_mean_flux,phot_rp_mean_flux_error,phot_rp_mean_flux_over_error,phot_rp_mean_mag,\n",
        "# phot_bp_rp_excess_factor,phot_proc_mode,bp_rp,bp_g,g_rp,radial_velocity,radial_velocity_error,\n",
        "# rv_nb_transits,rv_template_teff,rv_template_logg,rv_template_fe_h,phot_variable_flag,l,b,ecl_lon,\n",
        "# ecl_lat,priam_flags,teff_val,teff_percentile_lower,teff_percentile_upper,a_g_val,a_g_percentile_lower,\n",
        "# a_g_percentile_upper,e_bp_min_rp_val,e_bp_min_rp_percentile_lower,e_bp_min_rp_percentile_upper,\n",
        "# flame_flags,radius_val,radius_percentile_lower,radius_percentile_upper,lum_val,lum_percentile_lower,lum_percentile_upper\n",
        "\n",
        "df = pd.read_csv(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AothyCgL-h5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "2cbb47de-506c-472c-d423-1ffaabd54742"
      },
      "source": [
        "def selected_features (dataf, selectedf):\n",
        "  dataf = dataf.loc[:, selectedf]\n",
        "  return dataf\n",
        "\n",
        "# selected features\n",
        "selectedf = ['source_id', 'ra', 'ra_error', 'dec', 'dec_error', 'parallax', 'parallax_error', 'phot_g_mean_flux',\n",
        "                    'phot_g_mean_flux_error', 'phot_bp_mean_flux', 'phot_bp_mean_flux_error', 'phot_rp_mean_flux',\n",
        "                     'phot_rp_mean_flux_error','phot_g_mean_mag','phot_bp_mean_mag','phot_rp_mean_mag','bp_rp','bp_g','g_rp']\n",
        "gaia_df = selected_features(df, selectedf)\n",
        "gaia_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>ra</th>\n",
              "      <th>ra_error</th>\n",
              "      <th>dec</th>\n",
              "      <th>dec_error</th>\n",
              "      <th>parallax</th>\n",
              "      <th>parallax_error</th>\n",
              "      <th>phot_g_mean_flux</th>\n",
              "      <th>phot_g_mean_flux_error</th>\n",
              "      <th>phot_bp_mean_flux</th>\n",
              "      <th>phot_bp_mean_flux_error</th>\n",
              "      <th>phot_rp_mean_flux</th>\n",
              "      <th>phot_rp_mean_flux_error</th>\n",
              "      <th>phot_g_mean_mag</th>\n",
              "      <th>phot_bp_mean_mag</th>\n",
              "      <th>phot_rp_mean_mag</th>\n",
              "      <th>bp_rp</th>\n",
              "      <th>bp_g</th>\n",
              "      <th>g_rp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2449588108548044544</td>\n",
              "      <td>1.200590</td>\n",
              "      <td>0.336011</td>\n",
              "      <td>-1.101586</td>\n",
              "      <td>0.223228</td>\n",
              "      <td>0.148108</td>\n",
              "      <td>0.494231</td>\n",
              "      <td>451.860581</td>\n",
              "      <td>0.949830</td>\n",
              "      <td>268.504574</td>\n",
              "      <td>5.259502</td>\n",
              "      <td>299.713478</td>\n",
              "      <td>4.616114</td>\n",
              "      <td>19.050854</td>\n",
              "      <td>19.279009</td>\n",
              "      <td>18.570154</td>\n",
              "      <td>0.708855</td>\n",
              "      <td>0.228155</td>\n",
              "      <td>0.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2449591613241290624</td>\n",
              "      <td>1.010105</td>\n",
              "      <td>0.125334</td>\n",
              "      <td>-1.208384</td>\n",
              "      <td>0.080224</td>\n",
              "      <td>4.946350</td>\n",
              "      <td>0.176183</td>\n",
              "      <td>1795.014675</td>\n",
              "      <td>1.388877</td>\n",
              "      <td>373.834306</td>\n",
              "      <td>5.657353</td>\n",
              "      <td>2229.174080</td>\n",
              "      <td>5.786035</td>\n",
              "      <td>17.553196</td>\n",
              "      <td>18.919691</td>\n",
              "      <td>16.391560</td>\n",
              "      <td>2.528131</td>\n",
              "      <td>1.366495</td>\n",
              "      <td>1.161636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2449593021990602240</td>\n",
              "      <td>0.957800</td>\n",
              "      <td>0.265494</td>\n",
              "      <td>-1.147778</td>\n",
              "      <td>0.175054</td>\n",
              "      <td>2.377225</td>\n",
              "      <td>0.385595</td>\n",
              "      <td>556.364008</td>\n",
              "      <td>1.007956</td>\n",
              "      <td>110.273241</td>\n",
              "      <td>6.022923</td>\n",
              "      <td>733.751687</td>\n",
              "      <td>5.347473</td>\n",
              "      <td>18.824968</td>\n",
              "      <td>20.245213</td>\n",
              "      <td>17.598047</td>\n",
              "      <td>2.647165</td>\n",
              "      <td>1.420244</td>\n",
              "      <td>1.226921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2449595500187839360</td>\n",
              "      <td>0.901956</td>\n",
              "      <td>0.076135</td>\n",
              "      <td>-1.162254</td>\n",
              "      <td>0.048287</td>\n",
              "      <td>0.674556</td>\n",
              "      <td>0.106873</td>\n",
              "      <td>4121.808408</td>\n",
              "      <td>2.305190</td>\n",
              "      <td>1998.096403</td>\n",
              "      <td>8.629083</td>\n",
              "      <td>3059.473520</td>\n",
              "      <td>10.178913</td>\n",
              "      <td>16.650646</td>\n",
              "      <td>17.099848</td>\n",
              "      <td>16.047804</td>\n",
              "      <td>1.052044</td>\n",
              "      <td>0.449202</td>\n",
              "      <td>0.602842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2449586871597425152</td>\n",
              "      <td>1.223193</td>\n",
              "      <td>0.295541</td>\n",
              "      <td>-1.162055</td>\n",
              "      <td>0.176805</td>\n",
              "      <td>0.872118</td>\n",
              "      <td>0.405423</td>\n",
              "      <td>594.974435</td>\n",
              "      <td>1.086138</td>\n",
              "      <td>294.199064</td>\n",
              "      <td>7.992073</td>\n",
              "      <td>486.763098</td>\n",
              "      <td>5.330821</td>\n",
              "      <td>18.752119</td>\n",
              "      <td>19.179785</td>\n",
              "      <td>18.043627</td>\n",
              "      <td>1.136158</td>\n",
              "      <td>0.427666</td>\n",
              "      <td>0.708492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             source_id        ra  ra_error  ...     bp_rp      bp_g      g_rp\n",
              "0  2449588108548044544  1.200590  0.336011  ...  0.708855  0.228155  0.480700\n",
              "1  2449591613241290624  1.010105  0.125334  ...  2.528131  1.366495  1.161636\n",
              "2  2449593021990602240  0.957800  0.265494  ...  2.647165  1.420244  1.226921\n",
              "3  2449595500187839360  0.901956  0.076135  ...  1.052044  0.449202  0.602842\n",
              "4  2449586871597425152  1.223193  0.295541  ...  1.136158  0.427666  0.708492\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EoLSB-D_6hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select g, bp-g, bp-rp\n",
        "new_df = df.loc[:, ['ra', 'dec','phot_g_mean_mag', 'bp_g', 'bp_rp', 'parallax']]\n",
        "new_df = new_df.dropna()\n",
        "\n",
        "x_df = new_df.loc[:,['ra', 'dec','phot_g_mean_mag', 'bp_g', 'bp_rp']]\n",
        "y_df = new_df.loc[:,['parallax']]\n",
        "\n",
        "#split into train and validation\n",
        "ratio = 0.95 #the ratio is train/all \n",
        "msk = np.random.rand(len(x_df)) < ratio\n",
        "train_x = x_df[msk]\n",
        "train_y = y_df[msk]\n",
        "test_x = x_df[~msk]\n",
        "test_y = y_df[~msk]\n",
        "\n",
        "train_x = np.array(train_x).astype(float)\n",
        "train_y = np.array(train_y).astype(float)\n",
        "test_x = np.array(test_x).astype(float)\n",
        "test_y = np.array(test_y).astype(float)\n",
        "\n",
        "locations = np.random.choice(train_x.shape[0],\n",
        "                                 size=25000,\n",
        "                                 replace=False)\n",
        "\n",
        "#r_scaler = preprocessing.RobustScaler()\n",
        "train_norm_x = Normalization(train_x)[locations,:]\n",
        "#train_norm_x = Normalization(train_x)\n",
        "train_norm_y = Normalization(train_y)[locations,:]\n",
        "#train_norm_y = Normalization(train_y)\n",
        "#train_norm_y = r_scaler.fit_transform(train_y)\n",
        "test_norm_x = Normalization(test_x)\n",
        "test_norm_y = Normalization(test_y)\n",
        "#test_norm_y = r_scaler.fit_transform(test_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKws0fK7Rbwc",
        "colab_type": "text"
      },
      "source": [
        "### **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYDiwcPARdVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d73b08c6-e112-4971-d5f3-cf20a5e73490"
      },
      "source": [
        "TRAINING_ITERATIONS = 10000 #@param {type:\"number\"}\n",
        "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
        "PLOT_AFTER = 250 #@param {type:\"number\"}\n",
        "HIDDEN_SIZE = 256 #@param {type:\"number\"}\n",
        "MODEL_TYPE = 'ANP' #@param ['NP','ANP']\n",
        "ATTENTION_TYPE = 'multihead' #@param ['uniform','laplace','dot_product','multihead']\n",
        "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Sizes of the layers of the MLPs for the encoders and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
        "num_latents = HIDDEN_SIZE\n",
        "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
        "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
        "use_deterministic_path = True\n",
        "\n",
        "# ANP with multihead attention\n",
        "if MODEL_TYPE == 'ANP':\n",
        "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
        "                        att_type='multihead')\n",
        "# NP - equivalent to uniform attention\n",
        "elif MODEL_TYPE == 'NP':\n",
        "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
        "else:\n",
        "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
        "\n",
        "# Define the model\n",
        "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
        "                    decoder_output_sizes, use_deterministic_path, \n",
        "                    deterministic_encoder_output_sizes, attention)\n",
        "\n",
        "# Define data\n",
        "batch_size = 100\n",
        "train_query, train_target_y, train_num_total_points, train_num_context_points = datawrap(train_norm_x, train_norm_y, batch_size)\n",
        "\n",
        "test_query, test_target_y, test_num_total_points, test_num_context_points = testdatawrap(test_norm_x, test_norm_y, test_norm_x.shape[0])\n",
        "\n",
        "# Define the loss\n",
        "_, _, log_prob, _, loss = model(train_query, train_num_total_points,\n",
        "                                 train_target_y)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "mu, sigma, _, _, _ = model(test_query, test_num_total_points)\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-5)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Train and plot\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PLOT_AFTER == 0:\n",
        "      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, test_target_y, \n",
        "           test_query])\n",
        "      print(\"The accuracy is .....\")\n",
        "      accuracy = abs(pred_y-target_y)/target_y\n",
        "      accuracy = np.reshape(accuracy, accuracy.shape[1])\n",
        "      print(np.mean(accuracy))\n",
        "      \n",
        "      print(\"The prediction is ....\")\n",
        "      print(pred_y[:,:5,:])\n",
        "      print(\"The label is ....\")\n",
        "      print(target_y[:,:5,:])\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
        "\n",
        "      # Plot the prediction and the context\n",
        "      #plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "The accuracy is .....\n",
            "9.190585\n",
            "The prediction is ....\n",
            "[[[0.27974787]\n",
            "  [0.3095052 ]\n",
            "  [0.2739612 ]\n",
            "  [0.33054987]\n",
            "  [0.29710495]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 0, loss: 0.6584835648536682\n",
            "The accuracy is .....\n",
            "5.392028\n",
            "The prediction is ....\n",
            "[[[0.19925597]\n",
            "  [0.17626718]\n",
            "  [0.21930525]\n",
            "  [0.20182735]\n",
            "  [0.17941755]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 250, loss: -0.6127416491508484\n",
            "The accuracy is .....\n",
            "-0.15046585\n",
            "The prediction is ....\n",
            "[[[-0.00863851]\n",
            "  [ 0.0085392 ]\n",
            "  [-0.00077189]\n",
            "  [-0.01818252]\n",
            "  [-0.00973285]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 500, loss: -1.122978925704956\n",
            "The accuracy is .....\n",
            "1.8330811\n",
            "The prediction is ....\n",
            "[[[-0.04758922]\n",
            "  [-0.05576528]\n",
            "  [-0.03765562]\n",
            "  [-0.07904637]\n",
            "  [-0.07054719]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 750, loss: -1.2911012172698975\n",
            "The accuracy is .....\n",
            "0.049360786\n",
            "The prediction is ....\n",
            "[[[-0.0034579 ]\n",
            "  [-0.0131739 ]\n",
            "  [ 0.00322294]\n",
            "  [-0.03982793]\n",
            "  [-0.02134753]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 1000, loss: -1.3275986909866333\n",
            "The accuracy is .....\n",
            "0.90427846\n",
            "The prediction is ....\n",
            "[[[ 0.02230226]\n",
            "  [ 0.01958171]\n",
            "  [ 0.0289913 ]\n",
            "  [-0.01423452]\n",
            "  [ 0.00095449]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 1250, loss: -1.34487783908844\n",
            "The accuracy is .....\n",
            "-0.24506211\n",
            "The prediction is ....\n",
            "[[[-0.00764308]\n",
            "  [-0.00877011]\n",
            "  [-0.00074368]\n",
            "  [-0.03944711]\n",
            "  [-0.02904809]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 1500, loss: -1.3531453609466553\n",
            "The accuracy is .....\n",
            "-0.35098675\n",
            "The prediction is ....\n",
            "[[[-0.00645964]\n",
            "  [-0.0070182 ]\n",
            "  [ 0.00030652]\n",
            "  [-0.04137467]\n",
            "  [-0.0284202 ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 1750, loss: -1.35965096950531\n",
            "The accuracy is .....\n",
            "-0.10943218\n",
            "The prediction is ....\n",
            "[[[ 0.00134835]\n",
            "  [-0.00835822]\n",
            "  [ 0.01143146]\n",
            "  [-0.04055856]\n",
            "  [-0.02595645]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 2000, loss: -1.3609594106674194\n",
            "The accuracy is .....\n",
            "0.57231176\n",
            "The prediction is ....\n",
            "[[[ 0.01897254]\n",
            "  [ 0.01285945]\n",
            "  [ 0.02661724]\n",
            "  [-0.01994974]\n",
            "  [-0.00736429]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 2250, loss: -1.36282217502594\n",
            "The accuracy is .....\n",
            "-0.21535061\n",
            "The prediction is ....\n",
            "[[[ 0.00224825]\n",
            "  [-0.00484819]\n",
            "  [ 0.01038732]\n",
            "  [-0.03706627]\n",
            "  [-0.02552387]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 2500, loss: -1.3641692399978638\n",
            "The accuracy is .....\n",
            "-0.28301188\n",
            "The prediction is ....\n",
            "[[[ 0.00373606]\n",
            "  [-0.00107437]\n",
            "  [ 0.01086249]\n",
            "  [-0.03478388]\n",
            "  [-0.02261641]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 2750, loss: -1.3648321628570557\n",
            "The accuracy is .....\n",
            "-0.09152468\n",
            "The prediction is ....\n",
            "[[[ 0.00512346]\n",
            "  [ 0.0005568 ]\n",
            "  [ 0.01323278]\n",
            "  [-0.03363164]\n",
            "  [-0.0212105 ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 3000, loss: -1.3653303384780884\n",
            "The accuracy is .....\n",
            "-0.1754248\n",
            "The prediction is ....\n",
            "[[[ 0.00454093]\n",
            "  [-0.00134788]\n",
            "  [ 0.01232361]\n",
            "  [-0.03527457]\n",
            "  [-0.02268939]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 3250, loss: -1.3655996322631836\n",
            "The accuracy is .....\n",
            "-0.14375266\n",
            "The prediction is ....\n",
            "[[[ 0.00430092]\n",
            "  [-0.00197587]\n",
            "  [ 0.01218085]\n",
            "  [-0.03604246]\n",
            "  [-0.02343659]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 3500, loss: -1.3661482334136963\n",
            "The accuracy is .....\n",
            "-0.21935469\n",
            "The prediction is ....\n",
            "[[[ 0.00131564]\n",
            "  [-0.00455499]\n",
            "  [ 0.00943278]\n",
            "  [-0.03830231]\n",
            "  [-0.02551151]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 3750, loss: -1.3666027784347534\n",
            "The accuracy is .....\n",
            "-0.0021387392\n",
            "The prediction is ....\n",
            "[[[ 0.00297278]\n",
            "  [-0.00366356]\n",
            "  [ 0.01145438]\n",
            "  [-0.03516142]\n",
            "  [-0.02242176]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 4000, loss: -1.3671663999557495\n",
            "The accuracy is .....\n",
            "0.10184735\n",
            "The prediction is ....\n",
            "[[[ 0.000703  ]\n",
            "  [-0.00728824]\n",
            "  [ 0.00991714]\n",
            "  [-0.0300356 ]\n",
            "  [-0.019257  ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 4250, loss: -1.3686025142669678\n",
            "The accuracy is .....\n",
            "0.06327027\n",
            "The prediction is ....\n",
            "[[[-0.00073944]\n",
            "  [-0.01119793]\n",
            "  [ 0.01092867]\n",
            "  [-0.01564393]\n",
            "  [-0.01127374]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 4500, loss: -1.3701303005218506\n",
            "The accuracy is .....\n",
            "-0.18821476\n",
            "The prediction is ....\n",
            "[[[ 0.0018612 ]\n",
            "  [-0.00968882]\n",
            "  [ 0.01684043]\n",
            "  [-0.00350625]\n",
            "  [-0.00565718]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 4750, loss: -1.3704913854599\n",
            "The accuracy is .....\n",
            "-0.33455917\n",
            "The prediction is ....\n",
            "[[[ 0.00091547]\n",
            "  [-0.01090363]\n",
            "  [ 0.01814097]\n",
            "  [-0.00271526]\n",
            "  [-0.00645491]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 5000, loss: -1.3710848093032837\n",
            "The accuracy is .....\n",
            "-0.4278672\n",
            "The prediction is ....\n",
            "[[[-0.00026591]\n",
            "  [-0.01174903]\n",
            "  [ 0.01827244]\n",
            "  [-0.0035115 ]\n",
            "  [-0.00727338]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 5250, loss: -1.3714474439620972\n",
            "The accuracy is .....\n",
            "-0.40527546\n",
            "The prediction is ....\n",
            "[[[-0.00182735]\n",
            "  [-0.01303652]\n",
            "  [ 0.01763469]\n",
            "  [-0.00548754]\n",
            "  [-0.00850803]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 5500, loss: -1.371678113937378\n",
            "The accuracy is .....\n",
            "-0.44074303\n",
            "The prediction is ....\n",
            "[[[-0.0015273 ]\n",
            "  [-0.01249196]\n",
            "  [ 0.01873869]\n",
            "  [-0.00563274]\n",
            "  [-0.00797197]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 5750, loss: -1.3718329668045044\n",
            "The accuracy is .....\n",
            "-0.44629008\n",
            "The prediction is ....\n",
            "[[[-0.00165945]\n",
            "  [-0.01243241]\n",
            "  [ 0.01918946]\n",
            "  [-0.00631902]\n",
            "  [-0.00766863]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 6000, loss: -1.3720118999481201\n",
            "The accuracy is .....\n",
            "-0.37694424\n",
            "The prediction is ....\n",
            "[[[-0.003088  ]\n",
            "  [-0.0136631 ]\n",
            "  [ 0.01802098]\n",
            "  [-0.00869158]\n",
            "  [-0.00875027]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 6250, loss: -1.372024416923523\n",
            "The accuracy is .....\n",
            "-0.4084791\n",
            "The prediction is ....\n",
            "[[[-0.00209904]\n",
            "  [-0.01261467]\n",
            "  [ 0.01936689]\n",
            "  [-0.00835642]\n",
            "  [-0.00775249]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 6500, loss: -1.3722659349441528\n",
            "The accuracy is .....\n",
            "-0.3982718\n",
            "The prediction is ....\n",
            "[[[-0.00201356]\n",
            "  [-0.01238065]\n",
            "  [ 0.01967893]\n",
            "  [-0.00901642]\n",
            "  [-0.007485  ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 6750, loss: -1.3723719120025635\n",
            "The accuracy is .....\n",
            "-0.40076467\n",
            "The prediction is ....\n",
            "[[[-0.00148228]\n",
            "  [-0.01167636]\n",
            "  [ 0.020354  ]\n",
            "  [-0.00914661]\n",
            "  [-0.00688929]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 7000, loss: -1.3724714517593384\n",
            "The accuracy is .....\n",
            "-0.38934606\n",
            "The prediction is ....\n",
            "[[[-0.00046749]\n",
            "  [-0.01067984]\n",
            "  [ 0.02146764]\n",
            "  [-0.00875769]\n",
            "  [-0.00585003]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 7250, loss: -1.3725165128707886\n",
            "The accuracy is .....\n",
            "-0.3071043\n",
            "The prediction is ....\n",
            "[[[-0.0031378 ]\n",
            "  [-0.01305747]\n",
            "  [ 0.01872016]\n",
            "  [-0.01167888]\n",
            "  [-0.0081351 ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 7500, loss: -1.3724621534347534\n",
            "The accuracy is .....\n",
            "-0.30689088\n",
            "The prediction is ....\n",
            "[[[-0.00298136]\n",
            "  [-0.01283396]\n",
            "  [ 0.01896214]\n",
            "  [-0.0115566 ]\n",
            "  [-0.007835  ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 7750, loss: -1.3726199865341187\n",
            "The accuracy is .....\n",
            "-0.358891\n",
            "The prediction is ....\n",
            "[[[-0.0015757 ]\n",
            "  [-0.01131251]\n",
            "  [ 0.02039046]\n",
            "  [-0.01008591]\n",
            "  [-0.00632769]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 8000, loss: -1.3727571964263916\n",
            "The accuracy is .....\n",
            "-0.31452182\n",
            "The prediction is ....\n",
            "[[[-0.00280937]\n",
            "  [-0.01248968]\n",
            "  [ 0.01910388]\n",
            "  [-0.01122863]\n",
            "  [-0.00720348]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 8250, loss: -1.372744083404541\n",
            "The accuracy is .....\n",
            "-0.28832814\n",
            "The prediction is ....\n",
            "[[[-0.00348515]\n",
            "  [-0.01322506]\n",
            "  [ 0.01819895]\n",
            "  [-0.01183382]\n",
            "  [-0.0076559 ]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 8500, loss: -1.3727216720581055\n",
            "The accuracy is .....\n",
            "-0.38151422\n",
            "The prediction is ....\n",
            "[[[-0.00098125]\n",
            "  [-0.01078551]\n",
            "  [ 0.02059724]\n",
            "  [-0.00891515]\n",
            "  [-0.00509659]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 8750, loss: -1.3728398084640503\n",
            "The accuracy is .....\n",
            "-0.3452473\n",
            "The prediction is ....\n",
            "[[[-0.00213444]\n",
            "  [-0.01210192]\n",
            "  [ 0.01909101]\n",
            "  [-0.01009044]\n",
            "  [-0.00597173]]]\n",
            "The label is ....\n",
            "[[[ 0.02204271]\n",
            "  [-0.02888951]\n",
            "  [ 0.07062452]\n",
            "  [-0.0011733 ]\n",
            "  [-0.02867787]]]\n",
            "Iteration: 9000, loss: -1.3729286193847656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e2a061e69613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Plot the predictions in `PLOT_AFTER` intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1253\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNGC6wp7sd95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = abs(pred_y - target_y)/target_y\n",
        "print(mean(abs(pred_y - target_y)/target_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQREKYdvuRQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_bins = 20000\n",
        "n, bins, patches = plt.hist(acc[0], num_bins, facecolor='blue', alpha=0.5)\n",
        "plt.xlim(-2, 2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}