{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerDeepmind.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuaigezhu/starDist/blob/master/SoccerDeepmind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr99gs5dkcL4",
        "colab_type": "text"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAe8K9kGkDEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from math import pi\n",
        "from random import randint\n",
        "from torch import nn\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from sklearn import preprocessing\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oie0USHAlIf1",
        "colab_type": "text"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G953DAyGlLoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -q \"/content/fifadata.zip\"\n",
        "df = pd.read_csv('/content/data/data.csv')\n",
        "\n",
        "def numerical_value(string):\n",
        "    if string[-1]=='M':\n",
        "        return (float(string[1:-1])*1000000)\n",
        "    if string[-1]=='K':\n",
        "        return (float(string[1:-1])*1000)\n",
        "\n",
        "df['Value'] = df['Value'].apply(lambda x: numerical_value(x))\n",
        "\n",
        "df['ShortPassing'].fillna(df['ShortPassing'].mean(), inplace = True)\n",
        "df['Volleys'].fillna(df['Volleys'].mean(), inplace = True)\n",
        "df['Value'].fillna(df['Value'].mean(), inplace = True)\n",
        "df['Dribbling'].fillna(df['Dribbling'].mean(), inplace = True)\n",
        "df['Curve'].fillna(df['Curve'].mean(), inplace = True)\n",
        "df['FKAccuracy'].fillna(df['FKAccuracy'], inplace = True)\n",
        "df['LongPassing'].fillna(df['LongPassing'].mean(), inplace = True)\n",
        "df['BallControl'].fillna(df['BallControl'].mean(), inplace = True)\n",
        "df['HeadingAccuracy'].fillna(df['HeadingAccuracy'].mean(), inplace = True)\n",
        "df['Finishing'].fillna(df['Finishing'].mean(), inplace = True)\n",
        "df['Crossing'].fillna(df['Crossing'].mean(), inplace = True)\n",
        "df['Skill Moves'].fillna(df['Skill Moves'].median(), inplace = True)\n",
        "df['Weak Foot'].fillna(3, inplace = True)\n",
        "df['Preferred Foot'].fillna('Right', inplace = True)\n",
        "df['International Reputation'].fillna(1, inplace = True)\n",
        "\n",
        "features_numerical = ['Potential','Age','Weak Foot','Overall','Special', 'Skill Moves','International Reputation','Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes']\n",
        "X = df[features_numerical]\n",
        "Y = df['Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=10)\n",
        "X_test = X_test.reset_index()\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "X_train = X_train.reset_index()\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "\n",
        "r_scaler = preprocessing.RobustScaler()\n",
        "\n",
        "y_train = np.array(Y_train).reshape(Y_train.shape[0],-1)\n",
        "y_test = np.array(Y_test).reshape(Y_test.shape[0],-1)\n",
        "\n",
        "train_norm_x = Normalization(np.array(X_train))\n",
        "#train_norm_y = y_train\n",
        "train_norm_y = Normalization(np.array(y_train))\n",
        "test_norm_x = Normalization(np.array(X_test))\n",
        "#test_norm_y = y_test\n",
        "test_norm_y = Normalization(np.array(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZkfOAlFoC_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dd08659-50f2-4ff1-b8e7-505631371a6e"
      },
      "source": [
        "print(train_norm_x.shape)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14565, 41)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IOxx2DU-M73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)[:,:num_context+num_target,:]\n",
        "  target_y = np.array(batch_datay)[:,:num_context+num_target,:]\n",
        "  \n",
        "    #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = num_context+num_target\n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points\n",
        "\n",
        "def testdatawrap(data_x, data_y, batch_size):\n",
        "  num_target = 50\n",
        "  num_context = 50\n",
        "  batch_num = int(data_x.shape[0]/batch_size)\n",
        "  batch_datax = []\n",
        "  batch_datay = []\n",
        "  for i in range(batch_num):\n",
        "    locations = np.random.choice(data_x.shape[0],\n",
        "                                 size=batch_size,\n",
        "                                 replace=True)\n",
        "    tmp = data_x[locations,:]\n",
        "    batch_datax.append(tmp)\n",
        "    tmp = data_y[locations,:]\n",
        "    batch_datay.append(tmp)\n",
        "    #finish batching\n",
        "  context_x = np.array(batch_datax)[:,:num_context,:]#context is belong to target\n",
        "  context_y = np.array(batch_datay)[:,:num_context,:]\n",
        "  target_x = np.array(batch_datax)#x values\n",
        "  target_y = np.array(batch_datay)#y values\n",
        "  \n",
        "  #convert to tensor TF from np array\n",
        "  context_x = tf.convert_to_tensor(context_x, np.float32)\n",
        "  context_y = tf.convert_to_tensor(context_y, np.float32)\n",
        "  target_x = tf.convert_to_tensor(target_x, np.float32)\n",
        "  target_y = tf.convert_to_tensor(target_y, np.float32)\n",
        "  \n",
        "  query = ((context_x, context_y), target_x)\n",
        "  num_total_points = batch_size \n",
        "  num_context_points = num_context\n",
        "  return query, target_y, num_total_points, num_context_points\n",
        "\n",
        "#batch_size = 100\n",
        "#train_query, train_target_y, train_num_total_points, train_num_context_points = datawrap(train_norm_x, train_norm_y, batch_size)\n",
        "\n",
        "#test_query, test_target_y, test_num_total_points, test_num_context_points = testdatawrap(test_norm_x, test_norm_y, test_norm_x.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exh2BaDJ9BpJ",
        "colab_type": "text"
      },
      "source": [
        "### **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clErtHwF9ElT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def Normalization(data):\n",
        "  data_f = data.astype(float)\n",
        "  data_mean = np.mean(data_f, axis=0, keepdims=True)\n",
        "  data_n = data_f - data_mean\n",
        "  data_range = np.max(np.abs(data_n), axis=0, keepdims=True)\n",
        "  data_n = data_n / data_range\n",
        "  \n",
        "  return data_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXk_kIcO4Jk2",
        "colab_type": "text"
      },
      "source": [
        "### **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhCIvHMb4MyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_mlp(input, output_sizes, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "  batch_size, _, filter_size = input.shape.as_list()\n",
        "  output = tf.reshape(input, (-1, filter_size))\n",
        "  output.set_shape((None, filter_size))\n",
        "\n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(output_sizes[:-1]):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(\n",
        "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  # Bring back into original shape\n",
        "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxNLxEbF4xJq",
        "colab_type": "text"
      },
      "source": [
        "### **Deterministic Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8pnVoRT4yv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicEncoder(object):\n",
        "  \"\"\"The Deterministic Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, attention):\n",
        "    \"\"\"(A)NP deterministic encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      attention: The attention module.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._attention = attention\n",
        "\n",
        "  def __call__(self, context_x, context_y, target_x):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "      target_x: Tensor of shape [B,target_observations,d_x]. \n",
        "          For this 1D regression task this corresponds to the x-values.\n",
        "\n",
        "    Returns:\n",
        "      The encoded representation. Tensor of shape [B,target_observations,d]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \n",
        "                       \"deterministic_encoder\")\n",
        "\n",
        "    # Apply attention\n",
        "    with tf.variable_scope(\"deterministic_encoder\", reuse=tf.AUTO_REUSE):\n",
        "        hidden = self._attention(context_x, target_x, hidden)\n",
        "\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEJWEj1043Fz",
        "colab_type": "text"
      },
      "source": [
        "### **Latent Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd2sZRSI47if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentEncoder(object):\n",
        "  \"\"\"The Latent Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, num_latents):\n",
        "    \"\"\"(A)NP latent encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      num_latents: The latent dimensionality.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._num_latents = num_latents\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "\n",
        "    Returns:\n",
        "      A normal distribution over tensors of shape [B, num_latents]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([x, y], axis=-1)\n",
        "\n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \"latent_encoder\")\n",
        "      \n",
        "    # Aggregator: take the mean over all points\n",
        "    hidden = tf.reduce_mean(hidden, axis=1)\n",
        "    \n",
        "    # Have further MLP layers that map to the parameters of the Gaussian latent\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self._output_sizes[-1] + self._num_latents)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self._num_latents, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self._num_latents, name=\"std_layer\")\n",
        "      \n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy7DjMPJ5JCO",
        "colab_type": "text"
      },
      "source": [
        "### **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzTVQw2R5MLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes):\n",
        "    \"\"\"(A)NP decoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
        "          as defined in `basic.Linear`.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "\n",
        "  def __call__(self, representation, target_x):\n",
        "    \"\"\"Decodes the individual targets.\n",
        "\n",
        "    Args:\n",
        "      representation: The representation of the context for target predictions. \n",
        "          Tensor of shape [B,target_observations,?].\n",
        "      target_x: The x locations for the target query.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "\n",
        "    Returns:\n",
        "      dist: A multivariate Gaussian over the target points. A distribution over\n",
        "          tensors of shape [B,target_observations,d_y].\n",
        "      mu: The mean of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "      sigma: The standard deviation of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "    \"\"\"\n",
        "    # concatenate target_x and representation\n",
        "    hidden = tf.concat([representation, target_x], axis=-1)\n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(hidden, self._output_sizes, \"decoder\")\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "\n",
        "    # Get the distribution\n",
        "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
        "        loc=mu, scale_diag=sigma)\n",
        "\n",
        "    return dist, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFzmH-F25QHr",
        "colab_type": "text"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkLMFABL5SXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentModel(object):\n",
        "  \"\"\"The (A)NP model.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_encoder_output_sizes, num_latents,\n",
        "               decoder_output_sizes, use_deterministic_path=True, \n",
        "               deterministic_encoder_output_sizes=None, attention=None):\n",
        "    \"\"\"Initialises the model.\n",
        "\n",
        "    Args:\n",
        "      latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
        "          layers of the latent encoder.\n",
        "      num_latents: The latent dimensionality.\n",
        "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the decoder. The last element should correspond to d_y * 2\n",
        "          (it encodes both mean and variance concatenated)\n",
        "      use_deterministic_path: a boolean that indicates whether the deterministic\n",
        "          encoder is used or not.\n",
        "      deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
        "          hidden layers of the deterministic encoder. The last one is the size \n",
        "          of the deterministic representation r.\n",
        "      attention: The attention module used in the deterministic encoder.\n",
        "          Only relevant when use_deterministic_path=True.\n",
        "    \"\"\"\n",
        "    self._latent_encoder = LatentEncoder(latent_encoder_output_sizes, \n",
        "                                         num_latents)\n",
        "    self._decoder = Decoder(decoder_output_sizes)\n",
        "    self._use_deterministic_path = use_deterministic_path\n",
        "    if use_deterministic_path:\n",
        "      self._deterministic_encoder = DeterministicEncoder(\n",
        "          deterministic_encoder_output_sizes, attention)\n",
        "    \n",
        "\n",
        "  def __call__(self, query, num_targets, target_y=None):\n",
        "    \"\"\"Returns the predicted mean and variance at the target points.\n",
        "\n",
        "    Args:\n",
        "      query: Array containing ((context_x, context_y), target_x) where:\n",
        "          context_x: Tensor of shape [B,num_contexts,d_x]. \n",
        "              Contains the x values of the context points.\n",
        "          context_y: Tensor of shape [B,num_contexts,d_y]. \n",
        "              Contains the y values of the context points.\n",
        "          target_x: Tensor of shape [B,num_targets,d_x]. \n",
        "              Contains the x values of the target points.\n",
        "      num_targets: Number of target points.\n",
        "      target_y: The ground truth y values of the target y. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "\n",
        "    Returns:\n",
        "      log_p: The log_probability of the target_y given the predicted\n",
        "          distribution. Tensor of shape [B,num_targets].\n",
        "      mu: The mean of the predicted distribution. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "      sigma: The variance of the predicted distribution.\n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "    \"\"\"\n",
        "\n",
        "    (context_x, context_y), target_x = query\n",
        "\n",
        "    # Pass query through the encoder and the decoder\n",
        "    prior = self._latent_encoder(context_x, context_y)\n",
        "    \n",
        "    # For training, when target_y is available, use targets for latent encoder.\n",
        "    # Note that targets contain contexts by design.\n",
        "    if target_y is None:\n",
        "      latent_rep = prior.sample()\n",
        "    # For testing, when target_y unavailable, use contexts for latent encoder.\n",
        "    else:\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      latent_rep = posterior.sample()\n",
        "    latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n",
        "                         [1, num_targets, 1])\n",
        "    if self._use_deterministic_path:\n",
        "      deterministic_rep = self._deterministic_encoder(context_x, context_y,\n",
        "                                                      target_x)\n",
        "      representation = tf.concat([deterministic_rep, latent_rep], axis=-1)\n",
        "    else:\n",
        "      representation = latent_rep\n",
        "      \n",
        "    dist, mu, sigma = self._decoder(representation, target_x)\n",
        "    \n",
        "    # If we want to calculate the log_prob for training we will make use of the\n",
        "    # target_y. At test time the target_y is not available so we return None.\n",
        "    if target_y is not None:\n",
        "      log_p = dist.log_prob(target_y)\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      kl = tf.reduce_sum(\n",
        "          tf.contrib.distributions.kl_divergence(posterior, prior), \n",
        "          axis=-1, keepdims=True)\n",
        "      kl = tf.tile(kl, [1, num_targets])\n",
        "      loss = - tf.reduce_mean(log_p - kl / tf.cast(num_targets, tf.float32))\n",
        "    else:\n",
        "      log_p = None\n",
        "      kl = None\n",
        "      loss = None\n",
        "\n",
        "    return mu, sigma, log_p, kl, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMn5YLkX5Z4M",
        "colab_type": "text"
      },
      "source": [
        "### **Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqAeYLT05f6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uniform_attention(q, v):\n",
        "  \"\"\"Uniform attention. Equivalent to np.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  total_points = tf.shape(q)[1]\n",
        "  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
        "  rep = tf.tile(rep, [1, total_points, 1])\n",
        "  return rep\n",
        "\n",
        "def laplace_attention(q, k, v, scale, normalise):\n",
        "  \"\"\"Computes laplace exponential attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    scale: float that scales the L1 distance.\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  k = tf.expand_dims(k, axis=1)  # [B,1,n,d_k]\n",
        "  q = tf.expand_dims(q, axis=2)  # [B,m,1,d_k]\n",
        "  unnorm_weights = - tf.abs((k - q) / scale)  # [B,m,n,d_k]\n",
        "  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = lambda x: 1 + tf.tanh(x)\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def dot_product_attention(q, k, v, normalise):\n",
        "  \"\"\"Computes dot product attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = tf.shape(q)[-1]\n",
        "  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "  unnorm_weights = tf.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = tf.sigmoid\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def multihead_attention(q, k, v, num_heads=8):\n",
        "  \"\"\"Computes multi-head attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    num_heads: number of heads. Should divide d_v.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = q.get_shape().as_list()[-1]\n",
        "  d_v = v.get_shape().as_list()[-1]\n",
        "  head_size = d_v / num_heads\n",
        "  key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n",
        "  value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n",
        "  rep = tf.constant(0.0)\n",
        "  for h in range(num_heads):\n",
        "    o = dot_product_attention(\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wq%d' % h, use_bias=False, padding='VALID')(q),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wk%d' % h, use_bias=False, padding='VALID')(k),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wv%d' % h, use_bias=False, padding='VALID')(v),\n",
        "        normalise=True)\n",
        "    rep += tf.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n",
        "                      name='wo%d' % h, use_bias=False, padding='VALID')(o)\n",
        "  return rep\n",
        "\n",
        "class Attention(object):\n",
        "  \"\"\"The Attention module.\"\"\"\n",
        "\n",
        "  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n",
        "               num_heads=8):\n",
        "    \"\"\"Create attention module.\n",
        "\n",
        "    Takes in context inputs, target inputs and\n",
        "    representations of each context input/output pair\n",
        "    to output an aggregated representation of the context data.\n",
        "    Args:\n",
        "      rep: transformation to apply to contexts before computing attention. \n",
        "          One of: ['identity','mlp'].\n",
        "      output_sizes: list of number of hidden units per layer of mlp.\n",
        "          Used only if rep == 'mlp'.\n",
        "      att_type: type of attention. One of the following:\n",
        "          ['uniform','laplace','dot_product','multihead']\n",
        "      scale: scale of attention.\n",
        "      normalise: Boolean determining whether to:\n",
        "          1. apply softmax to weights so that they sum to 1 across context pts or\n",
        "          2. apply custom transformation to have weights in [0,1].\n",
        "      num_heads: number of heads for multihead.\n",
        "    \"\"\"\n",
        "    self._rep = rep\n",
        "    self._output_sizes = output_sizes\n",
        "    self._type = att_type\n",
        "    self._scale = scale\n",
        "    self._normalise = normalise\n",
        "    if self._type == 'multihead':\n",
        "      self._num_heads = num_heads\n",
        "\n",
        "  def __call__(self, x1, x2, r):\n",
        "    \"\"\"Apply attention to create aggregated representation of r.\n",
        "\n",
        "    Args:\n",
        "      x1: tensor of shape [B,n1,d_x].\n",
        "      x2: tensor of shape [B,n2,d_x].\n",
        "      r: tensor of shape [B,n1,d].\n",
        "      \n",
        "    Returns:\n",
        "      tensor of shape [B,n2,d]\n",
        "\n",
        "    Raises:\n",
        "      NameError: The argument for rep/type was invalid.\n",
        "    \"\"\"\n",
        "    if self._rep == 'identity':\n",
        "      k, q = (x1, x2)\n",
        "    elif self._rep == 'mlp':\n",
        "      # Pass through MLP\n",
        "      k = batch_mlp(x1, self._output_sizes, \"attention\")\n",
        "      q = batch_mlp(x2, self._output_sizes, \"attention\")\n",
        "    else:\n",
        "      raise NameError(\"'rep' not among ['identity','mlp']\")\n",
        "\n",
        "    if self._type == 'uniform':\n",
        "      rep = uniform_attention(q, r)\n",
        "    elif self._type == 'laplace':\n",
        "      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
        "    elif self._type == 'dot_product':\n",
        "      rep = dot_product_attention(q, k, r, self._normalise)\n",
        "    elif self._type == 'multihead':\n",
        "      rep = multihead_attention(q, k, r, self._num_heads)\n",
        "    else:\n",
        "      raise NameError((\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
        "                       \",'multihead']\"))\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnyRwGiH5o0u",
        "colab_type": "text"
      },
      "source": [
        "### **ANP training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Wubc2n5qnY",
        "colab_type": "code",
        "outputId": "1bf48654-0a36-4592-f14f-5d8defe6f9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TRAINING_ITERATIONS = 10000 #@param {type:\"number\"}\n",
        "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
        "PLOT_AFTER = 100 #@param {type:\"number\"}\n",
        "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
        "MODEL_TYPE = 'ANP' #@param ['NP','ANP']\n",
        "ATTENTION_TYPE = 'multihead' #@param ['uniform','laplace','dot_product','multihead']\n",
        "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Sizes of the layers of the MLPs for the encoders and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
        "num_latents = HIDDEN_SIZE\n",
        "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
        "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
        "use_deterministic_path = True\n",
        "\n",
        "# ANP with multihead attention\n",
        "if MODEL_TYPE == 'ANP':\n",
        "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
        "                        att_type='multihead')\n",
        "# NP - equivalent to uniform attention\n",
        "elif MODEL_TYPE == 'NP':\n",
        "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
        "else:\n",
        "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
        "\n",
        "# Define the model\n",
        "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
        "                    decoder_output_sizes, use_deterministic_path, \n",
        "                    deterministic_encoder_output_sizes, attention)\n",
        "\n",
        "# Define data\n",
        "batch_size = 100\n",
        "train_query, train_target_y, train_num_total_points, train_num_context_points = datawrap(train_norm_x, train_norm_y, batch_size)\n",
        "\n",
        "test_query, test_target_y, test_num_total_points, test_num_context_points = testdatawrap(test_norm_x, test_norm_y, test_norm_x.shape[0])\n",
        "\n",
        "# Define the loss\n",
        "_, _, log_prob, _, loss = model(train_query, train_num_total_points,\n",
        "                                 train_target_y)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "mu, sigma, _, _, _ = model(test_query, test_num_total_points)\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Train and plot\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PLOT_AFTER == 0:\n",
        "      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, test_target_y, \n",
        "           test_query])\n",
        "      print(\"The accuracy is .....\")\n",
        "      accuracy = abs(pred_y-target_y)/target_y\n",
        "      accuracy = np.reshape(accuracy, accuracy.shape[1])\n",
        "      print(np.median(accuracy))\n",
        "      \n",
        "      print(\"The prediction is ....\")\n",
        "      print(pred_y)\n",
        "      print(\"The label is ....\")\n",
        "      print(target_y)\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
        "\n",
        "      # Plot the prediction and the context\n",
        "      #plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "The accuracy is .....\n",
            "-2.8178363\n",
            "The prediction is ....\n",
            "[[[ 0.13212968]\n",
            "  [-0.15450421]\n",
            "  [ 0.08429081]\n",
            "  ...\n",
            "  [ 0.13798504]\n",
            "  [ 0.09604346]\n",
            "  [ 0.08613662]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 0, loss: 0.4944569170475006\n",
            "The accuracy is .....\n",
            "-0.78412795\n",
            "The prediction is ....\n",
            "[[[-0.01956134]\n",
            "  [ 0.11449046]\n",
            "  [-0.0177332 ]\n",
            "  ...\n",
            "  [-0.01172537]\n",
            "  [ 0.05038508]\n",
            "  [-0.00767634]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 100, loss: -1.3355348110198975\n",
            "The accuracy is .....\n",
            "-0.3820632\n",
            "The prediction is ....\n",
            "[[[-0.01612268]\n",
            "  [ 0.08839872]\n",
            "  [-0.01567603]\n",
            "  ...\n",
            "  [-0.01046952]\n",
            "  [ 0.03481272]\n",
            "  [-0.01462837]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 200, loss: -1.3720966577529907\n",
            "The accuracy is .....\n",
            "-0.6749064\n",
            "The prediction is ....\n",
            "[[[-0.01172036]\n",
            "  [ 0.0791742 ]\n",
            "  [-0.01182891]\n",
            "  ...\n",
            "  [-0.0039186 ]\n",
            "  [ 0.02746722]\n",
            "  [-0.00654504]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 300, loss: -1.3696942329406738\n",
            "The accuracy is .....\n",
            "-0.26302114\n",
            "The prediction is ....\n",
            "[[[-0.02908109]\n",
            "  [ 0.06473523]\n",
            "  [-0.02659555]\n",
            "  ...\n",
            "  [-0.02365157]\n",
            "  [ 0.02008159]\n",
            "  [-0.02697544]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 400, loss: -1.3768926858901978\n",
            "The accuracy is .....\n",
            "-0.13055158\n",
            "The prediction is ....\n",
            "[[[-0.01763284]\n",
            "  [ 0.07424257]\n",
            "  [-0.01872775]\n",
            "  ...\n",
            "  [-0.01423988]\n",
            "  [ 0.02609308]\n",
            "  [-0.01661715]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 500, loss: -1.3808259963989258\n",
            "The accuracy is .....\n",
            "-0.06455532\n",
            "The prediction is ....\n",
            "[[[-0.02409677]\n",
            "  [ 0.07570532]\n",
            "  [-0.0240337 ]\n",
            "  ...\n",
            "  [-0.01740524]\n",
            "  [ 0.02189871]\n",
            "  [-0.01951726]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 600, loss: -1.3803433179855347\n",
            "The accuracy is .....\n",
            "-0.050796825\n",
            "The prediction is ....\n",
            "[[[-0.02305118]\n",
            "  [ 0.07646188]\n",
            "  [-0.022638  ]\n",
            "  ...\n",
            "  [-0.01760451]\n",
            "  [ 0.02913831]\n",
            "  [-0.01947945]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 700, loss: -1.3821355104446411\n",
            "The accuracy is .....\n",
            "-0.40850934\n",
            "The prediction is ....\n",
            "[[[-0.01241383]\n",
            "  [ 0.06820795]\n",
            "  [-0.01046152]\n",
            "  ...\n",
            "  [-0.01146511]\n",
            "  [ 0.02184563]\n",
            "  [-0.01182705]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 800, loss: -1.3806780576705933\n",
            "The accuracy is .....\n",
            "-0.047739334\n",
            "The prediction is ....\n",
            "[[[-0.02077631]\n",
            "  [ 0.06336182]\n",
            "  [-0.01798372]\n",
            "  ...\n",
            "  [-0.0175255 ]\n",
            "  [ 0.01726911]\n",
            "  [-0.01866173]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 900, loss: -1.3819553852081299\n",
            "The accuracy is .....\n",
            "-0.061125666\n",
            "The prediction is ....\n",
            "[[[-0.02010439]\n",
            "  [ 0.07243773]\n",
            "  [-0.01871472]\n",
            "  ...\n",
            "  [-0.01642312]\n",
            "  [ 0.02297107]\n",
            "  [-0.01872734]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1000, loss: -1.3825410604476929\n",
            "The accuracy is .....\n",
            "-0.03532226\n",
            "The prediction is ....\n",
            "[[[-0.02258944]\n",
            "  [ 0.06946613]\n",
            "  [-0.02140841]\n",
            "  ...\n",
            "  [-0.01901487]\n",
            "  [ 0.01964485]\n",
            "  [-0.01877561]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1100, loss: -1.3826831579208374\n",
            "The accuracy is .....\n",
            "-0.158018\n",
            "The prediction is ....\n",
            "[[[-0.01885748]\n",
            "  [ 0.07240975]\n",
            "  [-0.01717486]\n",
            "  ...\n",
            "  [-0.01471963]\n",
            "  [ 0.02152581]\n",
            "  [-0.01592646]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1200, loss: -1.3827074766159058\n",
            "The accuracy is .....\n",
            "-0.32776487\n",
            "The prediction is ....\n",
            "[[[-0.02838415]\n",
            "  [ 0.06572687]\n",
            "  [-0.02661763]\n",
            "  ...\n",
            "  [-0.02403758]\n",
            "  [ 0.01433124]\n",
            "  [-0.02540817]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1300, loss: -1.380131483078003\n",
            "The accuracy is .....\n",
            "-0.11314784\n",
            "The prediction is ....\n",
            "[[[-0.01958264]\n",
            "  [ 0.07223208]\n",
            "  [-0.0188355 ]\n",
            "  ...\n",
            "  [-0.01500887]\n",
            "  [ 0.02114986]\n",
            "  [-0.01746039]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1400, loss: -1.3826218843460083\n",
            "The accuracy is .....\n",
            "-0.037929207\n",
            "The prediction is ....\n",
            "[[[-0.02194075]\n",
            "  [ 0.07380676]\n",
            "  [-0.01993371]\n",
            "  ...\n",
            "  [-0.01908611]\n",
            "  [ 0.02150987]\n",
            "  [-0.01969215]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1500, loss: -1.3828723430633545\n",
            "The accuracy is .....\n",
            "-0.21023983\n",
            "The prediction is ....\n",
            "[[[-0.02490569]\n",
            "  [ 0.05904203]\n",
            "  [-0.02324707]\n",
            "  ...\n",
            "  [-0.02175673]\n",
            "  [ 0.00888544]\n",
            "  [-0.02329028]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1600, loss: -1.3810584545135498\n",
            "The accuracy is .....\n",
            "-0.064096875\n",
            "The prediction is ....\n",
            "[[[-0.02140125]\n",
            "  [ 0.0703728 ]\n",
            "  [-0.01860522]\n",
            "  ...\n",
            "  [-0.01691136]\n",
            "  [ 0.01788744]\n",
            "  [-0.01770986]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1700, loss: -1.3829342126846313\n",
            "The accuracy is .....\n",
            "-0.07141581\n",
            "The prediction is ....\n",
            "[[[-0.02144624]\n",
            "  [ 0.07568124]\n",
            "  [-0.01958635]\n",
            "  ...\n",
            "  [-0.01582645]\n",
            "  [ 0.02252852]\n",
            "  [-0.01725443]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1800, loss: -1.382775068283081\n",
            "The accuracy is .....\n",
            "-0.0433018\n",
            "The prediction is ....\n",
            "[[[-0.02210588]\n",
            "  [ 0.06696769]\n",
            "  [-0.02231322]\n",
            "  ...\n",
            "  [-0.01809802]\n",
            "  [ 0.01392333]\n",
            "  [-0.01765857]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 1900, loss: -1.382966160774231\n",
            "The accuracy is .....\n",
            "-0.06423062\n",
            "The prediction is ....\n",
            "[[[-0.02402469]\n",
            "  [ 0.0711332 ]\n",
            "  [-0.02162031]\n",
            "  ...\n",
            "  [-0.01912055]\n",
            "  [ 0.01827236]\n",
            "  [-0.0198771 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2000, loss: -1.3826861381530762\n",
            "The accuracy is .....\n",
            "-0.09331262\n",
            "The prediction is ....\n",
            "[[[-0.01905021]\n",
            "  [ 0.07474404]\n",
            "  [-0.01829775]\n",
            "  ...\n",
            "  [-0.01604607]\n",
            "  [ 0.02385264]\n",
            "  [-0.01703948]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2100, loss: -1.3827712535858154\n",
            "The accuracy is .....\n",
            "-0.19022872\n",
            "The prediction is ....\n",
            "[[[-0.01813967]\n",
            "  [ 0.0809669 ]\n",
            "  [-0.01672205]\n",
            "  ...\n",
            "  [-0.01356118]\n",
            "  [ 0.02417997]\n",
            "  [-0.01575587]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2200, loss: -1.382370114326477\n",
            "The accuracy is .....\n",
            "-0.04010025\n",
            "The prediction is ....\n",
            "[[[-0.02095206]\n",
            "  [ 0.07310542]\n",
            "  [-0.01834344]\n",
            "  ...\n",
            "  [-0.01751797]\n",
            "  [ 0.01947677]\n",
            "  [-0.0194479 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2300, loss: -1.3830502033233643\n",
            "The accuracy is .....\n",
            "-0.25781858\n",
            "The prediction is ....\n",
            "[[[-0.01573747]\n",
            "  [ 0.08057121]\n",
            "  [-0.0140157 ]\n",
            "  ...\n",
            "  [-0.0126879 ]\n",
            "  [ 0.02703793]\n",
            "  [-0.01438668]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2400, loss: -1.381801962852478\n",
            "The accuracy is .....\n",
            "-0.08831521\n",
            "The prediction is ....\n",
            "[[[-0.02001393]\n",
            "  [ 0.07235955]\n",
            "  [-0.01952127]\n",
            "  ...\n",
            "  [-0.01609958]\n",
            "  [ 0.01607551]\n",
            "  [-0.01658504]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2500, loss: -1.3830785751342773\n",
            "The accuracy is .....\n",
            "-0.1258904\n",
            "The prediction is ....\n",
            "[[[-0.01945351]\n",
            "  [ 0.07699517]\n",
            "  [-0.01800663]\n",
            "  ...\n",
            "  [-0.01543392]\n",
            "  [ 0.02049017]\n",
            "  [-0.0162319 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2600, loss: -1.3830043077468872\n",
            "The accuracy is .....\n",
            "-0.09592638\n",
            "The prediction is ....\n",
            "[[[-0.02400436]\n",
            "  [ 0.07420824]\n",
            "  [-0.02229081]\n",
            "  ...\n",
            "  [-0.02009556]\n",
            "  [ 0.01810506]\n",
            "  [-0.02029728]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2700, loss: -1.3829818964004517\n",
            "The accuracy is .....\n",
            "-0.22652033\n",
            "The prediction is ....\n",
            "[[[-0.02520379]\n",
            "  [ 0.06984537]\n",
            "  [-0.02384431]\n",
            "  ...\n",
            "  [-0.02198078]\n",
            "  [ 0.01394617]\n",
            "  [-0.02391666]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2800, loss: -1.381091594696045\n",
            "The accuracy is .....\n",
            "-0.02868953\n",
            "The prediction is ....\n",
            "[[[-0.02078713]\n",
            "  [ 0.08101016]\n",
            "  [-0.01979857]\n",
            "  ...\n",
            "  [-0.01798926]\n",
            "  [ 0.02294909]\n",
            "  [-0.01868965]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 2900, loss: -1.3831371068954468\n",
            "The accuracy is .....\n",
            "-0.47310996\n",
            "The prediction is ....\n",
            "[[[-0.01345023]\n",
            "  [ 0.08923794]\n",
            "  [-0.01227227]\n",
            "  ...\n",
            "  [-0.00891737]\n",
            "  [ 0.03091675]\n",
            "  [-0.01020018]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3000, loss: -1.3790992498397827\n",
            "The accuracy is .....\n",
            "-0.05665324\n",
            "The prediction is ....\n",
            "[[[-0.02336574]\n",
            "  [ 0.07119594]\n",
            "  [-0.01977253]\n",
            "  ...\n",
            "  [-0.01842096]\n",
            "  [ 0.01469779]\n",
            "  [-0.01976121]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3100, loss: -1.3826823234558105\n",
            "The accuracy is .....\n",
            "-0.037770007\n",
            "The prediction is ....\n",
            "[[[-0.02215152]\n",
            "  [ 0.07679718]\n",
            "  [-0.02005582]\n",
            "  ...\n",
            "  [-0.01942939]\n",
            "  [ 0.01926786]\n",
            "  [-0.02012343]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3200, loss: -1.3831605911254883\n",
            "The accuracy is .....\n",
            "-0.059977643\n",
            "The prediction is ....\n",
            "[[[-0.02300297]\n",
            "  [ 0.07082602]\n",
            "  [-0.02031088]\n",
            "  ...\n",
            "  [-0.01859806]\n",
            "  [ 0.01122192]\n",
            "  [-0.02068966]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3300, loss: -1.3826314210891724\n",
            "The accuracy is .....\n",
            "-0.18750502\n",
            "The prediction is ....\n",
            "[[[-0.02461434]\n",
            "  [ 0.07053651]\n",
            "  [-0.02429508]\n",
            "  ...\n",
            "  [-0.02170379]\n",
            "  [ 0.01663652]\n",
            "  [-0.02300498]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3400, loss: -1.3821673393249512\n",
            "The accuracy is .....\n",
            "-0.31792915\n",
            "The prediction is ....\n",
            "[[[-0.02659076]\n",
            "  [ 0.06737292]\n",
            "  [-0.02506889]\n",
            "  ...\n",
            "  [-0.02371705]\n",
            "  [ 0.01291038]\n",
            "  [-0.02530132]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3500, loss: -1.3812943696975708\n",
            "The accuracy is .....\n",
            "-0.06697324\n",
            "The prediction is ....\n",
            "[[[-0.01894839]\n",
            "  [ 0.07604077]\n",
            "  [-0.01915636]\n",
            "  ...\n",
            "  [-0.01651057]\n",
            "  [ 0.01781019]\n",
            "  [-0.01802021]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3600, loss: -1.3832337856292725\n",
            "The accuracy is .....\n",
            "-0.036294345\n",
            "The prediction is ....\n",
            "[[[-0.02126317]\n",
            "  [ 0.07608265]\n",
            "  [-0.02056273]\n",
            "  ...\n",
            "  [-0.01914885]\n",
            "  [ 0.01730035]\n",
            "  [-0.02001222]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3700, loss: -1.3832051753997803\n",
            "The accuracy is .....\n",
            "-0.042182457\n",
            "The prediction is ....\n",
            "[[[-0.02146512]\n",
            "  [ 0.07570767]\n",
            "  [-0.02127617]\n",
            "  ...\n",
            "  [-0.0188676 ]\n",
            "  [ 0.01728665]\n",
            "  [-0.02032183]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3800, loss: -1.3831822872161865\n",
            "The accuracy is .....\n",
            "-0.13626236\n",
            "The prediction is ....\n",
            "[[[-0.01800087]\n",
            "  [ 0.08386084]\n",
            "  [-0.01714265]\n",
            "  ...\n",
            "  [-0.01486676]\n",
            "  [ 0.01944465]\n",
            "  [-0.01661601]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 3900, loss: -1.3830158710479736\n",
            "The accuracy is .....\n",
            "-1.7723727\n",
            "The prediction is ....\n",
            "[[[ 0.03094568]\n",
            "  [ 0.1671564 ]\n",
            "  [-0.01706676]\n",
            "  ...\n",
            "  [ 0.03122911]\n",
            "  [ 0.09202689]\n",
            "  [ 0.01945337]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4000, loss: -1.1675190925598145\n",
            "The accuracy is .....\n",
            "-0.0949835\n",
            "The prediction is ....\n",
            "[[[-0.02506934]\n",
            "  [ 0.09993952]\n",
            "  [-0.0186944 ]\n",
            "  ...\n",
            "  [-0.01996366]\n",
            "  [ 0.02856453]\n",
            "  [-0.02134656]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4100, loss: -1.3817112445831299\n",
            "The accuracy is .....\n",
            "-0.08101497\n",
            "The prediction is ....\n",
            "[[[-0.01945436]\n",
            "  [ 0.08116863]\n",
            "  [-0.01798819]\n",
            "  ...\n",
            "  [-0.01475305]\n",
            "  [ 0.02475029]\n",
            "  [-0.0164754 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4200, loss: -1.382343053817749\n",
            "The accuracy is .....\n",
            "-0.037473843\n",
            "The prediction is ....\n",
            "[[[-0.02341896]\n",
            "  [ 0.07158166]\n",
            "  [-0.02125235]\n",
            "  ...\n",
            "  [-0.01592345]\n",
            "  [ 0.01922778]\n",
            "  [-0.01778788]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4300, loss: -1.3827325105667114\n",
            "The accuracy is .....\n",
            "-0.20464078\n",
            "The prediction is ....\n",
            "[[[-0.01851409]\n",
            "  [ 0.07641488]\n",
            "  [-0.01708093]\n",
            "  ...\n",
            "  [-0.01243299]\n",
            "  [ 0.02284275]\n",
            "  [-0.01400429]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4400, loss: -1.3827128410339355\n",
            "The accuracy is .....\n",
            "-0.057418324\n",
            "The prediction is ....\n",
            "[[[-0.02076936]\n",
            "  [ 0.08385597]\n",
            "  [-0.02015618]\n",
            "  ...\n",
            "  [-0.01604704]\n",
            "  [ 0.02571866]\n",
            "  [-0.0175859 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4500, loss: -1.382156252861023\n",
            "The accuracy is .....\n",
            "-0.02602286\n",
            "The prediction is ....\n",
            "[[[-0.02145614]\n",
            "  [ 0.06748855]\n",
            "  [-0.02117826]\n",
            "  ...\n",
            "  [-0.0168158 ]\n",
            "  [ 0.01941081]\n",
            "  [-0.01842368]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4600, loss: -1.3830339908599854\n",
            "The accuracy is .....\n",
            "-0.03241996\n",
            "The prediction is ....\n",
            "[[[-0.02260431]\n",
            "  [ 0.07105997]\n",
            "  [-0.01571948]\n",
            "  ...\n",
            "  [-0.01542972]\n",
            "  [ 0.01813436]\n",
            "  [-0.01815146]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4700, loss: -1.3830584287643433\n",
            "The accuracy is .....\n",
            "-0.032477304\n",
            "The prediction is ....\n",
            "[[[-0.02074647]\n",
            "  [ 0.07636978]\n",
            "  [-0.01709597]\n",
            "  ...\n",
            "  [-0.01674684]\n",
            "  [ 0.02219673]\n",
            "  [-0.01847187]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4800, loss: -1.3830313682556152\n",
            "The accuracy is .....\n",
            "-0.026610674\n",
            "The prediction is ....\n",
            "[[[-0.02126421]\n",
            "  [ 0.07241294]\n",
            "  [-0.01576256]\n",
            "  ...\n",
            "  [-0.01749398]\n",
            "  [ 0.01881573]\n",
            "  [-0.01907153]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 4900, loss: -1.3831260204315186\n",
            "The accuracy is .....\n",
            "-0.10738829\n",
            "The prediction is ....\n",
            "[[[-0.02393332]\n",
            "  [ 0.07642388]\n",
            "  [-0.01895535]\n",
            "  ...\n",
            "  [-0.01966017]\n",
            "  [ 0.01996924]\n",
            "  [-0.02132878]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5000, loss: -1.3829869031906128\n",
            "The accuracy is .....\n",
            "-0.026045123\n",
            "The prediction is ....\n",
            "[[[-0.02092222]\n",
            "  [ 0.07260128]\n",
            "  [-0.01998106]\n",
            "  ...\n",
            "  [-0.01697204]\n",
            "  [ 0.0178338 ]\n",
            "  [-0.01874456]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5100, loss: -1.3831075429916382\n",
            "The accuracy is .....\n",
            "-0.120826684\n",
            "The prediction is ....\n",
            "[[[-0.01963062]\n",
            "  [ 0.08198965]\n",
            "  [-0.01684726]\n",
            "  ...\n",
            "  [-0.01418199]\n",
            "  [ 0.02155083]\n",
            "  [-0.01637411]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5200, loss: -1.3829185962677002\n",
            "The accuracy is .....\n",
            "-0.054538175\n",
            "The prediction is ....\n",
            "[[[-0.02049562]\n",
            "  [ 0.07855941]\n",
            "  [-0.01862655]\n",
            "  ...\n",
            "  [-0.01562941]\n",
            "  [ 0.02022248]\n",
            "  [-0.01770471]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5300, loss: -1.3832130432128906\n",
            "The accuracy is .....\n",
            "-0.26312608\n",
            "The prediction is ....\n",
            "[[[-0.01636494]\n",
            "  [ 0.08505033]\n",
            "  [-0.0134775 ]\n",
            "  ...\n",
            "  [-0.01246692]\n",
            "  [ 0.0259049 ]\n",
            "  [-0.01412399]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5400, loss: -1.381971836090088\n",
            "The accuracy is .....\n",
            "-0.06298576\n",
            "The prediction is ....\n",
            "[[[-0.02241384]\n",
            "  [ 0.06659369]\n",
            "  [-0.0194157 ]\n",
            "  ...\n",
            "  [-0.01876478]\n",
            "  [ 0.01450755]\n",
            "  [-0.02045341]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5500, loss: -1.3830088376998901\n",
            "The accuracy is .....\n",
            "-0.033154227\n",
            "The prediction is ....\n",
            "[[[-0.02041011]\n",
            "  [ 0.06951803]\n",
            "  [-0.01892671]\n",
            "  ...\n",
            "  [-0.01652848]\n",
            "  [ 0.01416616]\n",
            "  [-0.01823112]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5600, loss: -1.3832467794418335\n",
            "The accuracy is .....\n",
            "-0.049358036\n",
            "The prediction is ....\n",
            "[[[-0.02252086]\n",
            "  [ 0.06862737]\n",
            "  [-0.01970007]\n",
            "  ...\n",
            "  [-0.01867811]\n",
            "  [ 0.01369308]\n",
            "  [-0.02022327]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5700, loss: -1.3831794261932373\n",
            "The accuracy is .....\n",
            "-0.037128765\n",
            "The prediction is ....\n",
            "[[[-0.02013763]\n",
            "  [ 0.07732183]\n",
            "  [-0.01873852]\n",
            "  ...\n",
            "  [-0.01647313]\n",
            "  [ 0.01802308]\n",
            "  [-0.01810802]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5800, loss: -1.3832485675811768\n",
            "The accuracy is .....\n",
            "-0.026616018\n",
            "The prediction is ....\n",
            "[[[-0.0217656 ]\n",
            "  [ 0.07825124]\n",
            "  [-0.01899939]\n",
            "  ...\n",
            "  [-0.01744042]\n",
            "  [ 0.01830557]\n",
            "  [-0.01929011]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 5900, loss: -1.3832626342773438\n",
            "The accuracy is .....\n",
            "-0.0322286\n",
            "The prediction is ....\n",
            "[[[-0.0211423 ]\n",
            "  [ 0.07455248]\n",
            "  [-0.01713385]\n",
            "  ...\n",
            "  [-0.01727534]\n",
            "  [ 0.02037266]\n",
            "  [-0.01863642]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6000, loss: -1.383251667022705\n",
            "The accuracy is .....\n",
            "-0.057918698\n",
            "The prediction is ....\n",
            "[[[-0.01959344]\n",
            "  [ 0.07250711]\n",
            "  [-0.01963291]\n",
            "  ...\n",
            "  [-0.01594845]\n",
            "  [ 0.01774681]\n",
            "  [-0.01771523]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6100, loss: -1.3832507133483887\n",
            "The accuracy is .....\n",
            "-0.18432492\n",
            "The prediction is ....\n",
            "[[[-0.01691477]\n",
            "  [ 0.0778119 ]\n",
            "  [-0.01659572]\n",
            "  ...\n",
            "  [-0.0140857 ]\n",
            "  [ 0.02161121]\n",
            "  [-0.01570937]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6200, loss: -1.3823020458221436\n",
            "The accuracy is .....\n",
            "-0.051699158\n",
            "The prediction is ....\n",
            "[[[-0.02164405]\n",
            "  [ 0.06625522]\n",
            "  [-0.0212938 ]\n",
            "  ...\n",
            "  [-0.01880801]\n",
            "  [ 0.01109793]\n",
            "  [-0.01977131]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6300, loss: -1.382809042930603\n",
            "The accuracy is .....\n",
            "-0.042346362\n",
            "The prediction is ....\n",
            "[[[-0.01951464]\n",
            "  [ 0.07050537]\n",
            "  [-0.01891462]\n",
            "  ...\n",
            "  [-0.01690225]\n",
            "  [ 0.01659785]\n",
            "  [-0.01822933]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6400, loss: -1.3832780122756958\n",
            "The accuracy is .....\n",
            "-0.02842891\n",
            "The prediction is ....\n",
            "[[[-0.02036971]\n",
            "  [ 0.0725339 ]\n",
            "  [-0.01902511]\n",
            "  ...\n",
            "  [-0.01724958]\n",
            "  [ 0.01679172]\n",
            "  [-0.01883562]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6500, loss: -1.383305549621582\n",
            "The accuracy is .....\n",
            "-0.075123236\n",
            "The prediction is ....\n",
            "[[[-0.01939869]\n",
            "  [ 0.07549644]\n",
            "  [-0.01672913]\n",
            "  ...\n",
            "  [-0.01650156]\n",
            "  [ 0.01990674]\n",
            "  [-0.01809607]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6600, loss: -1.3831672668457031\n",
            "The accuracy is .....\n",
            "-0.05106829\n",
            "The prediction is ....\n",
            "[[[-0.01957392]\n",
            "  [ 0.06972633]\n",
            "  [-0.02096939]\n",
            "  ...\n",
            "  [-0.01646082]\n",
            "  [ 0.01657492]\n",
            "  [-0.01811471]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6700, loss: -1.3833402395248413\n",
            "The accuracy is .....\n",
            "-0.026036559\n",
            "The prediction is ....\n",
            "[[[-0.02044725]\n",
            "  [ 0.07199076]\n",
            "  [-0.02109791]\n",
            "  ...\n",
            "  [-0.01730659]\n",
            "  [ 0.01771998]\n",
            "  [-0.01859424]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6800, loss: -1.3833445310592651\n",
            "The accuracy is .....\n",
            "-0.06381777\n",
            "The prediction is ....\n",
            "[[[-0.01918101]\n",
            "  [ 0.0727226 ]\n",
            "  [-0.01814287]\n",
            "  ...\n",
            "  [-0.016756  ]\n",
            "  [ 0.01711568]\n",
            "  [-0.01824135]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 6900, loss: -1.3832695484161377\n",
            "The accuracy is .....\n",
            "-0.02528254\n",
            "The prediction is ....\n",
            "[[[-0.02009293]\n",
            "  [ 0.06559471]\n",
            "  [-0.02052897]\n",
            "  ...\n",
            "  [-0.01721103]\n",
            "  [ 0.01530433]\n",
            "  [-0.01870744]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7000, loss: -1.383329153060913\n",
            "The accuracy is .....\n",
            "-0.057261564\n",
            "The prediction is ....\n",
            "[[[-0.02131404]\n",
            "  [ 0.0696113 ]\n",
            "  [-0.01929404]\n",
            "  ...\n",
            "  [-0.01663647]\n",
            "  [ 0.01667231]\n",
            "  [-0.01771913]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7100, loss: -1.3824008703231812\n",
            "The accuracy is .....\n",
            "-0.12714034\n",
            "The prediction is ....\n",
            "[[[-0.02326321]\n",
            "  [ 0.06802271]\n",
            "  [-0.02195998]\n",
            "  ...\n",
            "  [-0.02022096]\n",
            "  [ 0.01437972]\n",
            "  [-0.02172451]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7200, loss: -1.3829636573791504\n",
            "The accuracy is .....\n",
            "-0.05775144\n",
            "The prediction is ....\n",
            "[[[-0.01953353]\n",
            "  [ 0.07102874]\n",
            "  [-0.01829628]\n",
            "  ...\n",
            "  [-0.01637337]\n",
            "  [ 0.01827696]\n",
            "  [-0.01777851]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7300, loss: -1.3832955360412598\n",
            "The accuracy is .....\n",
            "-0.33783847\n",
            "The prediction is ....\n",
            "[[[-0.01424656]\n",
            "  [ 0.07702553]\n",
            "  [-0.01454894]\n",
            "  ...\n",
            "  [-0.01142853]\n",
            "  [ 0.02445187]\n",
            "  [-0.01295721]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7400, loss: -1.3810021877288818\n",
            "The accuracy is .....\n",
            "-0.10363607\n",
            "The prediction is ....\n",
            "[[[-0.01803163]\n",
            "  [ 0.07306136]\n",
            "  [-0.01847851]\n",
            "  ...\n",
            "  [-0.01533709]\n",
            "  [ 0.01975867]\n",
            "  [-0.01682684]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7500, loss: -1.383255124092102\n",
            "The accuracy is .....\n",
            "-0.023346271\n",
            "The prediction is ....\n",
            "[[[-0.0207642 ]\n",
            "  [ 0.07093197]\n",
            "  [-0.01917745]\n",
            "  ...\n",
            "  [-0.01703466]\n",
            "  [ 0.0159387 ]\n",
            "  [-0.01859794]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7600, loss: -1.383373737335205\n",
            "The accuracy is .....\n",
            "-0.056650642\n",
            "The prediction is ....\n",
            "[[[-0.01939752]\n",
            "  [ 0.07026353]\n",
            "  [-0.01999026]\n",
            "  ...\n",
            "  [-0.01621735]\n",
            "  [ 0.01700718]\n",
            "  [-0.01771758]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7700, loss: -1.3833626508712769\n",
            "The accuracy is .....\n",
            "-0.027780462\n",
            "The prediction is ....\n",
            "[[[-0.02028479]\n",
            "  [ 0.06769611]\n",
            "  [-0.02012105]\n",
            "  ...\n",
            "  [-0.0178249 ]\n",
            "  [ 0.01429847]\n",
            "  [-0.01919491]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7800, loss: -1.3833856582641602\n",
            "The accuracy is .....\n",
            "-0.026131293\n",
            "The prediction is ....\n",
            "[[[-0.02115173]\n",
            "  [ 0.06568188]\n",
            "  [-0.02019532]\n",
            "  ...\n",
            "  [-0.01820698]\n",
            "  [ 0.01320257]\n",
            "  [-0.01956503]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 7900, loss: -1.3832842111587524\n",
            "The accuracy is .....\n",
            "-0.062978745\n",
            "The prediction is ....\n",
            "[[[-0.01926358]\n",
            "  [ 0.07060748]\n",
            "  [-0.01823129]\n",
            "  ...\n",
            "  [-0.01642496]\n",
            "  [ 0.0173495 ]\n",
            "  [-0.01792449]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8000, loss: -1.3833585977554321\n",
            "The accuracy is .....\n",
            "-0.025129352\n",
            "The prediction is ....\n",
            "[[[-0.0201368 ]\n",
            "  [ 0.0701854 ]\n",
            "  [-0.02004112]\n",
            "  ...\n",
            "  [-0.01768764]\n",
            "  [ 0.01616513]\n",
            "  [-0.01927477]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8100, loss: -1.3834011554718018\n",
            "The accuracy is .....\n",
            "-0.24967262\n",
            "The prediction is ....\n",
            "[[[-0.01484025]\n",
            "  [ 0.07613065]\n",
            "  [-0.01576373]\n",
            "  ...\n",
            "  [-0.01294119]\n",
            "  [ 0.02098072]\n",
            "  [-0.01451958]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8200, loss: -1.382079839706421\n",
            "The accuracy is .....\n",
            "-0.031182626\n",
            "The prediction is ....\n",
            "[[[-0.01983047]\n",
            "  [ 0.06530987]\n",
            "  [-0.01983956]\n",
            "  ...\n",
            "  [-0.0172872 ]\n",
            "  [ 0.01600417]\n",
            "  [-0.01869017]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8300, loss: -1.3833434581756592\n",
            "The accuracy is .....\n",
            "-0.03588795\n",
            "The prediction is ....\n",
            "[[[-0.0197405 ]\n",
            "  [ 0.07144484]\n",
            "  [-0.01937422]\n",
            "  ...\n",
            "  [-0.01716872]\n",
            "  [ 0.01568714]\n",
            "  [-0.01848517]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8400, loss: -1.3833836317062378\n",
            "The accuracy is .....\n",
            "-0.042100724\n",
            "The prediction is ....\n",
            "[[[-0.02128711]\n",
            "  [ 0.06492898]\n",
            "  [-0.02064651]\n",
            "  ...\n",
            "  [-0.01875602]\n",
            "  [ 0.01315814]\n",
            "  [-0.02049207]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8500, loss: -1.3832756280899048\n",
            "The accuracy is .....\n",
            "-0.0440945\n",
            "The prediction is ....\n",
            "[[[-0.02131173]\n",
            "  [ 0.06398241]\n",
            "  [-0.02123179]\n",
            "  ...\n",
            "  [-0.01864165]\n",
            "  [ 0.01239109]\n",
            "  [-0.02045152]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8600, loss: -1.3832943439483643\n",
            "The accuracy is .....\n",
            "-0.045635656\n",
            "The prediction is ....\n",
            "[[[-0.02149512]\n",
            "  [ 0.06610022]\n",
            "  [-0.02155801]\n",
            "  ...\n",
            "  [-0.01955424]\n",
            "  [ 0.01347437]\n",
            "  [-0.02078727]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8700, loss: -1.3832991123199463\n",
            "The accuracy is .....\n",
            "-0.050089817\n",
            "The prediction is ....\n",
            "[[[-0.02203151]\n",
            "  [ 0.06699566]\n",
            "  [-0.01801141]\n",
            "  ...\n",
            "  [-0.01885631]\n",
            "  [ 0.01373594]\n",
            "  [-0.02059765]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8800, loss: -1.3832005262374878\n",
            "The accuracy is .....\n",
            "-0.06426105\n",
            "The prediction is ....\n",
            "[[[-0.01890045]\n",
            "  [ 0.06900402]\n",
            "  [-0.01949034]\n",
            "  ...\n",
            "  [-0.01632503]\n",
            "  [ 0.01621296]\n",
            "  [-0.01779022]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 8900, loss: -1.3833717107772827\n",
            "The accuracy is .....\n",
            "-0.10234824\n",
            "The prediction is ....\n",
            "[[[-0.01838272]\n",
            "  [ 0.07019199]\n",
            "  [-0.01805547]\n",
            "  ...\n",
            "  [-0.01633152]\n",
            "  [ 0.01670614]\n",
            "  [-0.01728258]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9000, loss: -1.3832767009735107\n",
            "The accuracy is .....\n",
            "-0.05108661\n",
            "The prediction is ....\n",
            "[[[-0.01929143]\n",
            "  [ 0.07077014]\n",
            "  [-0.01889093]\n",
            "  ...\n",
            "  [-0.01689636]\n",
            "  [ 0.01617818]\n",
            "  [-0.01804261]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9100, loss: -1.38338041305542\n",
            "The accuracy is .....\n",
            "-0.03878368\n",
            "The prediction is ....\n",
            "[[[-0.01998084]\n",
            "  [ 0.07129011]\n",
            "  [-0.01841866]\n",
            "  ...\n",
            "  [-0.01705585]\n",
            "  [ 0.01585275]\n",
            "  [-0.01873907]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9200, loss: -1.3834160566329956\n",
            "The accuracy is .....\n",
            "-0.0603823\n",
            "The prediction is ....\n",
            "[[[-0.02253284]\n",
            "  [ 0.06862667]\n",
            "  [-0.02195666]\n",
            "  ...\n",
            "  [-0.02037262]\n",
            "  [ 0.01375654]\n",
            "  [-0.02120835]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9300, loss: -1.3832367658615112\n",
            "The accuracy is .....\n",
            "-0.020719133\n",
            "The prediction is ....\n",
            "[[[-0.02113781]\n",
            "  [ 0.07008642]\n",
            "  [-0.0203146 ]\n",
            "  ...\n",
            "  [-0.01828179]\n",
            "  [ 0.01351189]\n",
            "  [-0.01933091]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9400, loss: -1.3833283185958862\n",
            "The accuracy is .....\n",
            "-0.027625132\n",
            "The prediction is ....\n",
            "[[[-0.02011919]\n",
            "  [ 0.06850236]\n",
            "  [-0.02063933]\n",
            "  ...\n",
            "  [-0.01802263]\n",
            "  [ 0.01657424]\n",
            "  [-0.01901637]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9500, loss: -1.3834383487701416\n",
            "The accuracy is .....\n",
            "-0.035689637\n",
            "The prediction is ....\n",
            "[[[-0.02225031]\n",
            "  [ 0.07049707]\n",
            "  [-0.02126294]\n",
            "  ...\n",
            "  [-0.01837574]\n",
            "  [ 0.01619741]\n",
            "  [-0.01939789]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9600, loss: -1.3834017515182495\n",
            "The accuracy is .....\n",
            "-0.022260947\n",
            "The prediction is ....\n",
            "[[[-0.02049412]\n",
            "  [ 0.06692844]\n",
            "  [-0.02001949]\n",
            "  ...\n",
            "  [-0.01802136]\n",
            "  [ 0.01402962]\n",
            "  [-0.0189913 ]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9700, loss: -1.383418083190918\n",
            "The accuracy is .....\n",
            "-0.022596663\n",
            "The prediction is ....\n",
            "[[[-0.02063548]\n",
            "  [ 0.06734665]\n",
            "  [-0.02009602]\n",
            "  ...\n",
            "  [-0.01782005]\n",
            "  [ 0.01490398]\n",
            "  [-0.01899859]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9800, loss: -1.3834335803985596\n",
            "The accuracy is .....\n",
            "-0.026340012\n",
            "The prediction is ....\n",
            "[[[-0.02040501]\n",
            "  [ 0.06885467]\n",
            "  [-0.01996229]\n",
            "  ...\n",
            "  [-0.01749633]\n",
            "  [ 0.01478441]\n",
            "  [-0.01866866]]]\n",
            "The label is ....\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "Iteration: 9900, loss: -1.3834569454193115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4lsCmbkx2aA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "616cd541-0dbf-4a99-bfe3-1f046762242e"
      },
      "source": [
        "print(pred_y)\n",
        "print(target_y)\n",
        "print (np.median((abs(pred_y-target_y)/target_y)))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[-0.02040501]\n",
            "  [ 0.06885467]\n",
            "  [-0.01996229]\n",
            "  ...\n",
            "  [-0.01749633]\n",
            "  [ 0.01478441]\n",
            "  [-0.01866866]]]\n",
            "[[[-0.02081715]\n",
            "  [ 0.00077112]\n",
            "  [-0.02090962]\n",
            "  ...\n",
            "  [-0.01697949]\n",
            "  [ 0.01700459]\n",
            "  [-0.01882896]]]\n",
            "-0.026340012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-B86LQks_V",
        "colab_type": "text"
      },
      "source": [
        "### **Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzV9PKJ2kxR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_functions(target_x, target_y, context_x, context_y, pred_y, std):\n",
        "  \"\"\"Plots the predicted mean and variance and the context points.\n",
        "  \n",
        "  Args: \n",
        "    target_x: An array of shape [B,num_targets,1] that contains the\n",
        "        x values of the target points.\n",
        "    target_y: An array of shape [B,num_targets,1] that contains the\n",
        "        y values of the target points.\n",
        "    context_x: An array of shape [B,num_contexts,1] that contains \n",
        "        the x values of the context points.\n",
        "    context_y: An array of shape [B,num_contexts,1] that contains \n",
        "        the y values of the context points.\n",
        "    pred_y: An array of shape [B,num_targets,1] that contains the\n",
        "        predicted means of the y values at the target points in target_x.\n",
        "    std: An array of shape [B,num_targets,1] that contains the\n",
        "        predicted std dev of the y values at the target points in target_x.\n",
        "  \"\"\"\n",
        "  # Plot everything\n",
        "  #plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
        "  #plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
        "  \n",
        "  #plt.plot(context_x[0], context_y[0], 'ko', markersize=1)\n",
        "  \n",
        "  #plt.fill_between(\n",
        "      #target_x[0, :, 0],\n",
        "      #pred_y[0, :, 0] - std[0, :, 0],\n",
        "      #pred_y[0, :, 0] + std[0, :, 0],\n",
        "      #alpha=0.2,\n",
        "      #facecolor='#65c9f7',\n",
        "      #interpolate=True)\n",
        "\n",
        "  # Make the plot pretty\n",
        "  plt.grid('off')\n",
        "  ax = plt.gca()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}